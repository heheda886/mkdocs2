{"config":{"lang":["zh"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"ABFML","text":"<p>ABFML (A problem-oriented package for rapidly creating, screening, and optimizing new machine learning force fields) is a Python-based toolkit designed to streamline the development, training, and deployment of machine learning force fields (MLFFs). It integrates modern deep learning frameworks and molecular dynamics engines, enabling efficient model construction and application.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>Rapid construction of MLFF models based on physical descriptors</li> <li>Support for large-scale molecular dynamics via LAMMPS and ASE</li> <li>Modular design for easy extension and research customization</li> </ul>"},{"location":"#quickstart","title":"Quickstart","text":"<p><pre><code>pip install abfml\n</code></pre> For more details, check the Installation Guide and Tutorials.</p>"},{"location":"#documentation","title":"Documentation","text":"<ul> <li>Installation</li> <li>Tutorials</li> <li>API Reference</li> <li>Citation</li> </ul>"},{"location":"#citation","title":"Citation","text":"<p>If you use ABFML in your research, please cite:</p> <p>Xingze Geng, Jianing Gu, Gaowu Qin, Lin-Wang Wang, Xiangying Meng. ABFML: A problem-oriented package for rapidly creating, screening, and optimizing new machine learning force fields. J. Chem. Phys. 162, 052502 (2025). https://doi.org/10.1063/5.0247559</p>"},{"location":"citation/","title":"Citation","text":"<p>If you use ABFML in your research, please cite the following paper:</p> <p>Xingze Geng, Jianing Gu, Gaowu Qin, Lin-Wang Wang, Xiangying Meng. ABFML: A problem-oriented package for rapidly creating, screening, and optimizing new machine learning force fields. Journal of Chemical Physics 162, 052502 (2025). https://doi.org/10.1063/5.0247559</p>"},{"location":"citation/#bibtex","title":"BibTeX","text":"<pre><code>@article{ABFML2025,\n  author  = {Xingze Geng and Jianing Gu and Gaowu Qin and Lin-Wang Wang and Xiangying Meng},\n  title   = {ABFML: A problem-oriented package for rapidly creating, screening, and optimizing new machine learning force fields},\n  journal = {Journal of Chemical Physics},\n  year    = {2025},\n  volume  = {162},\n  number  = {5},\n  pages   = {052502},\n  doi     = {10.1063/5.0247559}\n}\n</code></pre>"},{"location":"cli/","title":"Command-Line Interface (CLI)","text":"<p>ABFML provides a command-line interface for quick model checking, training, and validation. You can display general help using: <pre><code>abfml --help\n</code></pre></p> <p>Check the validity and inference capability of a trained model. <pre><code>abfml check -m model.pt -d float32\n</code></pre> - Purpose: Ensures the model file can be loaded and used for inference.</p>"},{"location":"cli/#train","title":"<code>train</code>","text":"<p>Start training a machine learning force field model based on a configuration file. <pre><code>abfml train input.json\n</code></pre> - Purpose: Launches the training process using parameters defined in <code>input.json</code>.</p>"},{"location":"cli/#valid","title":"<code>valid</code>","text":"<p>Validate a trained model using a test dataset. <pre><code>abfml valid -m model.pt -f \"../data/test.extxyz\" -n 10\n</code></pre> - Purpose: Evaluates model performance with sp</p>"},{"location":"cli/#available-commands","title":"Available Commands","text":""},{"location":"cli/#checkecified-test-data-and-sample-size","title":"<code>check</code>ecified test data and sample size.","text":""},{"location":"cli/#help-for-commands","title":"Help for Commands","text":"<p>Each command provides detailed parameter information using <code>-h</code>: <pre><code>abfml train -h\nabfml valid -h\nabfml check -h\n</code></pre></p>"},{"location":"communicate/","title":"\u7559\u8a00\u677f","text":"Communicate"},{"location":"install/","title":"Installation","text":""},{"location":"install/#1-installing-abfml","title":"1. Installing ABFML","text":"<p>To quickly get started with ABFML, set up an isolated Python environment and install the package.</p> <pre><code># 1. Create a new conda environment\nconda create -n abfml python=3.11\n\n# 2. Activate the environment\nconda activate abfml\n\n# 3. Install ABFML\n\n# Option 1: Install from source (recommended for development)\ncd path/to/ABFML\npip install .\n\n# Option 2: Install from PyPI (for general use)\npip install abfml\n</code></pre>"},{"location":"install/#2-installing-abfml-lammps-optional","title":"2. Installing ABFML-LAMMPS (Optional)","text":"<p>This step is only required if you plan to run large-scale molecular dynamics simulations  with LAMMPS using ABFML-generated force fields. For initial model development and testing, ABFML works directly with ASE and you can skip this section.</p>"},{"location":"install/#21-prerequisites","title":"2.1 Prerequisites","text":"<ol> <li> <p>LAMMPS    Download and build LAMMPS for your platform: LAMMPS Official Website</p> </li> <li> <p>Libtorch    Download the PyTorch C++ API (Libtorch).    If your system's toolchain is older, use the Pre-cxx11 ABI version.</p> </li> </ol>"},{"location":"install/#22-integration-steps","title":"2.2 Integration Steps","text":""},{"location":"install/#1-copy-abfml-files-into-lammps","title":"1. Copy ABFML files into LAMMPS","text":"<pre><code>cp abfml/lmp/ABFML lammps/src\ncp abfml/lmp/Makefile.mpi lammps/src/MAKE\n</code></pre>"},{"location":"install/#2-modify-makefilempi","title":"2. Modify <code>Makefile.mpi</code>","text":"<p>Edit <code>lammps/src/MAKE/Makefile.mpi</code> to match your system paths for compilers and libraries: <pre><code>CCFLAGS += -I/PATH/libtorch/include/\nCCFLAGS += -I/PATH/libtorch/include/torch/csrc/api/include/\nCCFLAGS += -I/PATH/libtorch\nLINKFLAGS += -L/PATH/libtorch/lib/ -ltorch -lc10 -ltorch_cpu\n</code></pre></p>"},{"location":"install/#3-build-lammps-with-abfml","title":"3. Build LAMMPS with ABFML","text":"<pre><code>export LD_LIBRARY_PATH=/path/libtorch/lib:$LD_LIBRARY_PATH\ncd lammps/src\nmake yes-abfml\nmake mpi\n</code></pre>"},{"location":"install/#23-running-lammps-with-abfml","title":"2.3 Running LAMMPS with ABFML","text":"<p>Example of using ABFML in an input script: <pre><code>pair_style abfml model.pt\npair_coeff * * 29 30\n</code></pre> Here, element numbers (e.g., <code>29</code>, <code>30</code>) represent element types.</p>"},{"location":"install/#notes","title":"Notes","text":"<ul> <li>Ensure that the Libtorch version matches the PyTorch version used for training the ABFML model.</li> <li>Always verify the correct <code>LD_LIBRARY_PATH</code> before running LAMMPS.</li> </ul>"},{"location":"api/data/","title":"abfml.data","text":""},{"location":"api/data/#abfml.data.read_data.ABFMLDataset","title":"<code>ABFMLDataset</code>","text":"<p>               Bases: <code>Dataset</code></p> Source code in <code>abfml\\data\\dataset.py</code> <pre><code>class ABFMLDataset(Dataset):\n    def __init__(\n        self,\n        num_frames: int,\n        num_atoms: int,\n        element_types: torch.Tensor,\n        central_atoms: torch.Tensor,\n        neighbor_indices: torch.Tensor,\n        neighbor_types: torch.Tensor,\n        neighbor_vectors: torch.Tensor,\n        energy: torch.Tensor = None,\n        atomic_energies: torch.Tensor = None,\n        forces: torch.Tensor = None,\n        virials: torch.Tensor = None,\n    ):\n        super().__init__()\n        self.num_frames = num_frames\n        self.num_atoms = num_atoms\n\n        # Validate physical quantities\n        physics_info = {\n            \"energy\": (energy, (num_frames, 1)),\n            \"atomic_energies\": (atomic_energies, (num_frames, num_atoms, 1)),\n            \"forces\": (forces, (num_frames, num_atoms, 3)),\n            \"virials\": (virials, (num_frames, 9)),\n        }\n\n        for name, (tensor, expected_shape) in physics_info.items():\n            if tensor is not None:\n                if not isinstance(tensor, torch.Tensor):\n                    raise TypeError(f\"{name} must be a torch.Tensor, but got {type(tensor)}.\")\n                if tensor.shape != expected_shape:\n                    raise ValueError(f\"{name} must have shape {expected_shape}, but got {tensor.shape}.\")\n\n        self.energy = energy\n        self.atomic_energies = atomic_energies\n        self.forces = forces\n        self.virials = virials\n\n        # Validate neighbor-related tensors\n        num_neighbors = neighbor_indices.shape[-1]\n        neighbor_info = {\n            \"central_atoms\": (central_atoms, (num_frames, num_atoms)),\n            \"neighbor_indices\": (neighbor_indices, (num_frames, num_atoms, num_neighbors)),\n            \"neighbor_types\": (neighbor_types, (num_frames, num_atoms, num_neighbors)),\n            \"neighbor_vectors\": (neighbor_vectors, (num_frames, num_atoms, num_neighbors, 4)),\n        }\n\n        for name, (tensor, expected_shape) in neighbor_info.items():\n            if not isinstance(tensor, torch.Tensor):\n                raise TypeError(f\"{name} must be a torch.Tensor, but got {type(tensor)}.\")\n            if tensor.shape != expected_shape:\n                raise ValueError(f\"{name} must have shape {expected_shape}, but got {tensor.shape}.\")\n\n        self.element_types = element_types\n        self.central_atoms = central_atoms\n        self.neighbor_indices = neighbor_indices\n        self.neighbor_types = neighbor_types\n        self.neighbor_vectors = neighbor_vectors\n\n    def __len__(self):\n        return self.num_frames\n\n    def __getitem__(self, index):\n        sample = {\n            \"num_atoms\": self.num_atoms,\n            \"element_types\": self.element_types,\n            \"central_atoms\": self.central_atoms[index],\n            \"neighbor_indices\": self.neighbor_indices[index],\n            \"neighbor_types\": self.neighbor_types[index],\n            \"neighbor_vectors\": self.neighbor_vectors[index],\n        }\n\n        if self.energy is not None:\n            sample[\"energy\"] = self.energy[index]\n        if self.atomic_energies is not None:\n            sample[\"atomic_energies\"] = self.atomic_energies[index]\n        if self.forces is not None:\n            sample[\"forces\"] = self.forces[index]\n        if self.virials is not None:\n            sample[\"virials\"] = self.virials[index]\n\n        return sample\n</code></pre>"},{"location":"api/data/#abfml.data.read_data.DataClass","title":"<code>DataClass</code>  <code>dataclass</code>","text":"Source code in <code>abfml\\data\\read_data.py</code> <pre><code>@dataclass\nclass DataClass:\n    filename: str\n    atoms_list: List[Atoms]\n    include_element: set = None\n    include_atoms_numbers: set = None\n    num_frames: int = 0\n    def __init__(self, filename: str, file_format: Optional[str]):\n        self.filename: str = filename\n        if file_format == \"pwmat-config\":\n            atoms_list = []\n        elif file_format == \"pwmat-movement\":\n            atoms_list = []\n        else:\n            atoms_list = io.read(filename, format=file_format, index=':')\n        self.atoms_list: List[Atoms] = atoms_list\n        self.include_element: set = set()\n        include_atoms_number = []\n        for atom_information in atoms_list:\n            self.include_element = self.include_element.union(set(atom_information.get_chemical_symbols()))\n            include_atoms_number.append(len(atom_information))\n        self.include_atoms_number: set = set(include_atoms_number)\n        self.n_frames: int = len(self.atoms_list)\n</code></pre>"},{"location":"api/data/#abfml.data.read_data.ReadData","title":"<code>ReadData</code>","text":"Source code in <code>abfml\\data\\read_data.py</code> <pre><code>class ReadData:\n    def __init__(\n            self,\n            filename: Union[str, List[str]],\n            cutoff: float,\n            neighbor: Union[Dict[int, int], int],\n            file_format: Optional[str]):\n        self.cutoff = cutoff\n        self.neighbor = neighbor\n        self.filename = filename\n        self.atoms_list: list[Atoms] = []\n        if isinstance(filename, str):\n            filename = [filename]\n        self.data_information: List[dict] = []\n        for file in filename:\n            data_information = DataClass(file, file_format)\n            self.data_information.append({'file_name': data_information.filename,\n                                          'n_frames': data_information.n_frames,\n                                          'include_element': data_information.include_element,\n                                          'include_atoms_number': data_information.include_atoms_number})\n            self.atoms_list = self.atoms_list + data_information.atoms_list\n        self.n_frames: int = len(self.atoms_list)\n        self.unique_image = {}\n        for i, image in enumerate(self.atoms_list):\n            image_tuple = tuple(image.get_atomic_numbers())\n            if image_tuple in self.unique_image:\n                self.unique_image[image_tuple].append(i)\n            else:\n                self.unique_image[image_tuple] = [i]\n\n    def create_dataset(self):\n        dataset = []\n        for same_image_index in self.unique_image.values():\n            n_frames: int = len(same_image_index)\n            n_atoms: int = len(self.atoms_list[same_image_index[0]])\n            n_elements: int = len(set(self.atoms_list[same_image_index[0]].get_atomic_numbers()))\n            if isinstance(self.neighbor, int):\n                max_neighbor: int = self.neighbor\n            elif isinstance(self.neighbor, dict):\n                max_neighbor = sum(self.neighbor[atomic_numbers]\n                                       for atomic_numbers in set(self.atoms_list[same_image_index[0]].get_atomic_numbers()))\n            else:\n                raise ValueError(f'Expected a member of Dict[int, int] and int but instead found type {type(self.neighbor)}')\n\n            central_atoms = torch.empty(n_frames, n_atoms, dtype=torch.int)\n            neighbor_indices = torch.empty(n_frames, n_atoms, max_neighbor, dtype=torch.int)\n            neighbor_types = torch.empty(n_frames, n_atoms, max_neighbor, dtype=torch.int)\n            neighbor_vectors = torch.zeros(n_frames, n_atoms, max_neighbor, 4, dtype=torch.float64)\n            element_types = torch.zeros(n_frames, n_elements, dtype=torch.int)\n            energy = torch.zeros(n_frames, 1, dtype=torch.float64)\n            atomic_energies = torch.zeros(n_frames, n_atoms, 1, dtype=torch.float64)\n            forces = torch.zeros(n_frames, n_atoms, 3, dtype=torch.float64)\n            virials = torch.zeros(n_frames, 9, dtype=torch.float64)\n            for i, image_index in enumerate(same_image_index):\n                atoms_ase = self.atoms_list[image_index]\n                neighbor_information = ReadData.calculate_neighbor(atoms=atoms_ase,\n                                                                   cutoff=self.cutoff,\n                                                                   neighbor=self.neighbor)\n                element_types = torch.tensor(neighbor_information[0])\n                central_atoms[i] = torch.tensor(neighbor_information[1])\n                neighbor_indices[i] = torch.tensor(neighbor_information[2])\n                neighbor_types[i] = torch.tensor(neighbor_information[3])\n                neighbor_vectors[i] = torch.tensor(neighbor_information[4])\n                try:\n                    energy[i] = torch.tensor(atoms_ase.get_potential_energy())\n                except (RuntimeError, AttributeError):\n                    energy = None\n                try:\n                    atomic_energies[i] = torch.tensor(atoms_ase.atomic_energies)\n                except (RuntimeError, AttributeError):\n                    atomic_energies = None\n                try:\n                    forces[i] = torch.tensor(atoms_ase.get_forces(apply_constraint=False))\n                except (RuntimeError, AttributeError):\n                    forces = None\n                try:\n                    # ASE calculated in units of eV/A^3, virial: eV\n                    virials[i] = torch.tensor(-1.0 * atoms_ase.get_stress(voigt=False).reshape(9) * atoms_ase.get_volume())\n                except (RuntimeError, AttributeError):\n                    virials = None\n            dataset.append(ABFMLDataset(num_frames=n_frames,\n                                        num_atoms=n_atoms,\n                                        element_types=element_types,\n                                        central_atoms=central_atoms,\n                                        neighbor_indices=neighbor_indices,\n                                        neighbor_types=neighbor_types,\n                                        neighbor_vectors=neighbor_vectors,\n                                        energy=energy,\n                                        atomic_energies=atomic_energies,\n                                        forces=forces,\n                                        virials=virials))\n        return dataset\n\n    @staticmethod\n    def calculate_neighbor(atoms: Atoms,\n                           cutoff: float,\n                           neighbor: Union[Dict[int, int], int]):\n        nl = NeighborList([cutoff / 2] * len(atoms), skin=0, self_interaction=False, bothways=True, sorted=False)\n        nl.update(atoms)\n        atoms_num = len(atoms)\n        central_atoms = atoms.numbers\n        element_types = np.unique(central_atoms)\n        if isinstance(neighbor, int):\n            max_neighbor = neighbor\n            neighbor_vectors = np.zeros(shape=(1, atoms_num, max_neighbor, 4), dtype=np.float64)\n            neighbor_indices = np.full(shape=(1, atoms_num, max_neighbor), fill_value=-1, dtype=np.int32)\n            neighbor_types = np.full(shape=(1, atoms_num, max_neighbor), fill_value=-1, dtype=np.int32)\n            for i in range(atoms_num):\n                indices_neighbor, offsets = nl.get_neighbors(i)\n                neighbor_positions = atoms.positions[indices_neighbor] + np.dot(offsets, atoms.get_cell())\n                delta = neighbor_positions - atoms.positions[i]\n                rij = np.linalg.norm(delta, axis=1)\n                indices = np.arange(indices_neighbor.shape[0])\n                if indices_neighbor.shape[0] &lt;= max_neighbor:\n                    sorted_indices = indices\n                else:\n                    order = np.argsort(rij, axis=-1)\n                    sorted_indices = indices[order[:max_neighbor]]\n                valid_count = len(sorted_indices)\n\n                neighbor_vectors[:, i, :valid_count, 1:] = delta[sorted_indices, :]\n                neighbor_vectors[:, i, :valid_count, 0] = rij[sorted_indices]\n                neighbor_indices[:, i, :valid_count] = indices_neighbor[sorted_indices]\n            mask = (neighbor_indices != -1)\n            neighbor_types[mask] = central_atoms[neighbor_indices[mask]]\n\n        elif isinstance(neighbor, Dict):\n            max_neighbor = sum(neighbor[element] for element in element_types)\n            slot_map = {}\n            current_slot = 0\n            for atomic_number in neighbor.keys():\n                slot_size = neighbor[atomic_number]\n                slot_map[atomic_number] = (current_slot, current_slot + slot_size)\n                current_slot += slot_size\n            central_atoms = atoms.numbers\n            neighbor_vectors = np.zeros(shape=(1, atoms_num, max_neighbor, 4), dtype=np.float64)\n            neighbor_indices = np.full(shape=(1, atoms_num, max_neighbor), fill_value=-1, dtype=np.int32)\n            neighbor_types = np.full(shape=(1, atoms_num, max_neighbor), fill_value=-1, dtype=np.int32)\n            for i in range(atoms_num):\n                indices_neighbor, offsets = nl.get_neighbors(i)\n                neighbor_positions = atoms.positions[indices_neighbor] + np.dot(offsets, atoms.get_cell())\n                delta = neighbor_positions - atoms.positions[i]\n                rij = np.linalg.norm(delta, axis=1)\n\n                for atomic_numbers in np.unique(element_types):\n                    mask = (central_atoms[indices_neighbor])==atomic_numbers\n                    if not np.any(mask):\n                        continue\n                    start_slot, end_slot = slot_map[atomic_numbers]\n                    max_slots = end_slot - start_slot\n                    valid_indices = np.where(mask)[0]\n                    valid_rij = rij[valid_indices]\n                    if valid_indices.shape[0] &lt;= max_slots:\n                        sorted_indices = valid_indices\n                    else:\n                        order = np.argsort(valid_rij, axis=-1)\n                        sorted_indices = valid_indices[order[:max_slots]]\n                    valid_count = len(sorted_indices)\n                    slot_slice = slice(start_slot, start_slot + valid_count)\n                    neighbor_vectors[:, i, slot_slice, 1:] = delta[sorted_indices, :]\n                    neighbor_vectors[:, i, slot_slice, 0] = rij[sorted_indices]\n                    neighbor_indices[:, i, slot_slice] = indices_neighbor[sorted_indices]\n            mask = (neighbor_indices != -1)\n            neighbor_types[mask] = central_atoms[neighbor_indices[mask]]\n        else:\n            raise Exception('neighbor[0] and neighbor[1] should have the same length '\n                            'or neighbor[1] have only one element. Maybe you should read the manual!')\n        central_atoms = central_atoms[np.newaxis, ...]\n        return element_types, central_atoms, neighbor_indices, neighbor_types, neighbor_vectors\n</code></pre>"},{"location":"api/data/#abfml.data.read_data.to_graph","title":"<code>to_graph(central_atoms, neighbor_indices, neighbor_types, neighbor_vectors)</code>","text":"<p>Convert atomic neighbor information into a list of PyG graph objects (one per batch entry).</p> <p>Parameters:</p> Name Type Description Default <code>central_atoms</code> <code>Tensor</code> <p>Shape (B, N), atomic types of central atoms.</p> required <code>neighbor_indices</code> <code>Tensor</code> <p>Shape (B, N, M), indices of neighbors (-1 for invalid).</p> required <code>neighbor_types</code> <code>Tensor</code> <p>Shape (B, N, M), types of neighboring atoms.</p> required <code>neighbor_vectors</code> <code>Tensor</code> <p>Shape (B, N, M, 4), relative vectors to neighbors (r, dx, dy, dz).</p> required <p>Notes: For the safety of subsequent operations, this out returns a list instead of a Batch of torch_geometric.data. Returns:     List[Data]: A list of torch_geometric.data.Data graphs, one for each batch item.     Each Data object contains:         - x (Tensor): Node features, shape [num_nodes, num_node_features]. Here, it's the atomic type.         - edge_index (LongTensor): Graph connectivity in COO format, shape [2, num_edges].         - relative_pos (Tensor): Edge features (e.g., distances and relative positions), shape [num_edges, 4].</p> Source code in <code>abfml\\data\\read_data.py</code> <pre><code>def to_graph(central_atoms: torch.Tensor,\n             neighbor_indices: torch.Tensor,\n             neighbor_types: torch.Tensor,\n             neighbor_vectors: torch.Tensor) -&gt; List[Data]:\n    \"\"\"\n    Convert atomic neighbor information into a list of PyG graph objects (one per batch entry).\n\n    Args:\n        central_atoms (Tensor): Shape (B, N), atomic types of central atoms.\n        neighbor_indices (Tensor): Shape (B, N, M), indices of neighbors (-1 for invalid).\n        neighbor_types (Tensor): Shape (B, N, M), types of neighboring atoms.\n        neighbor_vectors (Tensor): Shape (B, N, M, 4), relative vectors to neighbors (r, dx, dy, dz).\n\n    Notes: For the safety of subsequent operations, this out returns a list instead of a Batch of torch_geometric.data.\n    Returns:\n        List[Data]: A list of torch_geometric.data.Data graphs, one for each batch item.\n        Each Data object contains:\n            - x (Tensor): Node features, shape [num_nodes, num_node_features]. Here, it's the atomic type.\n            - edge_index (LongTensor): Graph connectivity in COO format, shape [2, num_edges].\n            - relative_pos (Tensor): Edge features (e.g., distances and relative positions), shape [num_edges, 4].\n    \"\"\"\n    batch, n_atoms, max_neighbor = neighbor_types.shape\n    device, dtype = neighbor_vectors.device, neighbor_vectors.dtype\n    graphs = []\n\n    for b in range(batch):\n        # Create node features from central atom types (shape: [N, 1])\n        x = central_atoms[b].unsqueeze(-1).to(torch.long)\n\n        # Identify valid neighbor connections (exclude -1 entries)\n        valid_mask = neighbor_indices[b] != -1\n\n        # Get valid (source_atom_index, neighbor_index_in_M) pairs\n        src_idx, nbr_idx = valid_mask.nonzero(as_tuple=True)\n\n        # Source atoms are the indices from 0 to N-1 (repeated)\n        edge_src = src_idx\n\n        # Destination atoms are the valid neighbor indices\n        edge_dst = neighbor_indices[b][src_idx, nbr_idx]\n\n        # Stack to form edge_index of shape [2, num_edges]\n        edge_index = torch.stack([edge_src, edge_dst], dim=0).to(device)\n\n        # Extract relative vectors (r, dx, dy, dz)\n        relative_pos = neighbor_vectors[b][src_idx, nbr_idx].to(device=device, dtype=dtype)\n        # Construct a PyG Data object for this graph\n        graph = Data(x=x, edge_index=edge_index, relative_pos=relative_pos)\n        graphs.append(graph)\n    return graphs\n</code></pre>"},{"location":"api/utils/","title":"abfml.utils","text":""},{"location":"api/utils/#abfml.utils.chart.plot_error_distribution","title":"<code>plot_error_distribution(difference, predict_key, rmse, r2, unit)</code>","text":"<p>Plot error distribution using histogram and KDE.</p> Source code in <code>abfml\\utils\\chart.py</code> <pre><code>def plot_error_distribution(difference, predict_key, rmse, r2, unit):\n    \"\"\"\n    Plot error distribution using histogram and KDE.\n    \"\"\"\n    plt.figure(figsize=(10, 8))\n\n    # Histogram part\n    counts, bins, _ = plt.hist(difference, bins=40, density=True, alpha=0.7, color='#1f77b4', edgecolor='black')\n\n    # KDE (Kernel Density Estimation) smoothed density curve\n    kde = gaussian_kde(difference)\n    kde_x = np.linspace(bins.min(), bins.max(), 300)\n    plt.plot(kde_x, kde(kde_x), color='red', linewidth=2, label='Density')\n\n    # Annotate RMSE and R\u00b2 values\n    plt.text(0.65, 0.95, f\"RMSE: {rmse:.4f}{unit}\\nR\u00b2: {r2:.4f}\", transform=plt.gca().transAxes,\n             fontsize=18, verticalalignment='top', bbox=dict(facecolor='white', alpha=0.8))\n\n    # Set title, axis labels, and grid\n    plt.title(f'{predict_key} Error Distribution', fontsize=22)\n    plt.xlabel('Error (DFT - Predict)', fontsize=20)\n    plt.ylabel('Density', fontsize=20)\n    plt.tick_params(axis='both', labelsize=18)\n    plt.grid(True, linestyle='--', alpha=0.5)\n    plt.legend()\n\n    # Save error distribution plot\n    plt.savefig(f'ErrorDistributionOf_{predict_key}.png', dpi=300, bbox_inches='tight')\n    plt.close()\n</code></pre>"},{"location":"api/utils/#abfml.utils.chart.plot_loss_curve","title":"<code>plot_loss_curve(file_name)</code>","text":"<p>Reads a loss file and plots the training and validation RMSE curves.</p> <p>Parameters:</p> Name Type Description Default <code>file_name</code> <code>str</code> <p>Path to the input CSV file.</p> required Source code in <code>abfml\\utils\\chart.py</code> <pre><code>def plot_loss_curve(file_name):\n    \"\"\"\n    Reads a loss file and plots the training and validation RMSE curves.\n\n    Parameters:\n        file_name (str): Path to the input CSV file.\n    \"\"\"\n    # Read the data file\n    data_pd = pd.read_csv(file_name, sep=r'\\s+', header=0)\n\n    # List of RMSE metrics to be plotted\n    label_list = [\"Loss\", \"E_tot\", \"Force\", \"Ei\", \"Virial\"]\n\n    for label in label_list:\n        train_key = f\"T_RMSE_{label}\"\n        valid_key = f\"V_RMSE_{label}\"\n\n        # Check if both training and validation keys exist in the dataset\n        if train_key in data_pd and valid_key in data_pd:\n            # Create a figure and axis with a larger size for better readability\n            fig, ax = plt.subplots(figsize=(12, 8))\n\n            # Plot training and validation RMSE curves\n            ax.plot(data_pd[\"Epoch\"], data_pd[train_key], label=\"Train\", linestyle=\"-\", marker=\"o\",\n                    markersize=4)\n            ax.plot(data_pd[\"Epoch\"], data_pd[valid_key], label=\"Valid\", linestyle=\"--\", marker=\"s\",\n                    markersize=4)\n\n            # Set title and axis labels\n            ax.set_title(f\"{label} RMSE vs Epoch\", fontsize=18)\n            ax.set_xlabel(\"Epoch\", fontsize=18)\n            ax.set_ylabel(f\"{label} RMSE\", fontsize=18)\n            ax.tick_params(axis='both', labelsize=16)\n\n            # Add grid lines with transparency\n            ax.grid(linestyle=\"--\", alpha=0.6)\n\n            # Improve legend visibility\n            ax.legend(fontsize=18, loc=\"best\", frameon=True, edgecolor=\"gray\")\n\n            # Save the figure as a high-resolution PNG file\n            plt.savefig(f'loss_{label}.png', dpi=300, bbox_inches=\"tight\")\n            plt.close()\n</code></pre>"},{"location":"api/utils/#abfml.utils.chart.plot_scatter","title":"<code>plot_scatter(dft, predict, predict_key, rmse, r2, unit)</code>","text":"<p>Plot scatter diagram comparing DFT and predicted values.</p> Source code in <code>abfml\\utils\\chart.py</code> <pre><code>def plot_scatter(dft, predict, predict_key, rmse, r2, unit):\n    \"\"\"\n    Plot scatter diagram comparing DFT and predicted values.\n    \"\"\"\n    plt.figure(figsize=(10, 8))  # Increase image size\n    plt.scatter(dft, predict, s=20, label='Data', alpha=0.6)  # Enhance scatter plot effect\n\n    # Diagonal line (y = x, ideal fit line)\n    data_min = min(dft.min(), predict.min())\n    data_max = max(dft.max(), predict.max())\n    plt.plot([data_min, data_max], [data_min, data_max], color='black', linestyle='--', linewidth=1.5, label='$y=x$')\n\n    # Annotate RMSE and R\u00b2 values\n    plt.text(0.05, 0.95, f\"RMSE: {rmse:.4f}{unit}\\nR\u00b2: {r2:.4f}\", transform=plt.gca().transAxes,\n             fontsize=18, verticalalignment='top', bbox=dict(facecolor='white', alpha=0.8))\n\n    # Set title, axis labels, and grid\n    plt.title(f'{predict_key}: DFT vs Predict', fontsize=22)\n    plt.xlabel(f'{predict_key}$_{{DFT}}$', fontsize=20)\n    plt.ylabel(f'{predict_key}$_{{Predict}}$', fontsize=20)\n    plt.xlim(data_min, data_max)\n    plt.ylim(data_min, data_max)\n    plt.tick_params(axis='both', labelsize=18)\n    plt.grid(True, linestyle='--', alpha=0.5)\n    plt.legend()\n\n    # Save scatter plot\n    plt.savefig(f'{predict_key}_scatter.png', dpi=300, bbox_inches='tight')\n    plt.close()\n</code></pre>"},{"location":"api/utils/#abfml.utils.code_tree.print_tree","title":"<code>print_tree(root_dir, file=None, prefix='')</code>","text":"Source code in <code>abfml\\utils\\code_tree.py</code> <pre><code>def print_tree(root_dir, file=None, prefix=''):\n    items = sorted(os.listdir(root_dir))\n    for i, name in enumerate(items):\n        path = os.path.join(root_dir, name)\n        is_last = (i == len(items) - 1)\n        branch = '\u2514\u2500\u2500 ' if is_last else '\u251c\u2500\u2500 '\n        line = f\"{prefix}{branch}{name}\"\n        print(line)\n        if file:\n            file.write(line + '\\n')\n        if os.path.isdir(path):\n            extension = '    ' if is_last else '\u2502   '\n            print_tree(path, file=file, prefix=prefix + extension)\n</code></pre>"},{"location":"api/core/loss/","title":"abfml.core.loss","text":""},{"location":"api/core/loss/#abfml.core.loss.losser.LossSet","title":"<code>LossSet = create_dataclass(loss_set_schema)</code>  <code>module-attribute</code>","text":""},{"location":"api/core/loss/#abfml.core.loss.losser.AverageMeter","title":"<code>AverageMeter</code>","text":"<p>               Bases: <code>object</code></p> <p>Computes and stores the average and current value</p> Source code in <code>abfml\\logger\\loggers.py</code> <pre><code>class AverageMeter(object):\n    \"\"\"Computes and stores the average and current value\"\"\"\n    def __init__(self, name: str, fmt: str = \".4e\", summary_type: str = \"\"):\n        self.name = name\n        self.fmt = fmt\n        summary_type_list = [\"NONE\", \"AVERAGE\", \"SUM\", \"COUNT\"]\n        if summary_type in summary_type_list:\n            self.summary_type = summary_type\n        else:\n            raise ValueError(f\"Invalid summary type : {summary_type}, supported only {summary_type_list}\")\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n: int = 1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n    def __str__(self):\n        fmtstr = f\"{self.name}: {self.val:{self.fmt}}\"\n        return fmtstr.format(**self.__dict__)\n\n    def summary(self):\n        fmtstr = \"\"\n        if self.summary_type == \"NONE\":\n            fmtstr = f\"{self.name}: {self.val:{self.fmt}}\"\n        elif self.summary_type == \"AVERAGE\":\n            fmtstr = f\"{self.name}: {self.avg:{self.fmt}}\"\n        elif self.summary_type == \"SUM\":\n            fmtstr = f\"{self.name}: {self.sum:{self.fmt}}\"\n        elif self.summary_type == \"COUNT\":\n            fmtstr = f\"{self.name}: {self.count:{self.fmt}}\"\n        else:\n            raise ValueError(\"Invalid summary type %r\" % self.summary_type)\n        return fmtstr\n</code></pre>"},{"location":"api/core/loss/#abfml.core.loss.losser.calculate_weight","title":"<code>calculate_weight(param, learn_rate)</code>","text":"Source code in <code>abfml\\core\\loss\\losser.py</code> <pre><code>def calculate_weight(param: LossSet, learn_rate: float):\n    attributes = ['energy',  'ei', 'force', 'virial']\n    weights = []\n\n    for attr in attributes:\n        start_weight = getattr(param, f'start_{attr}_weight')\n        limit_weight = getattr(param, f'limit_{attr}_weight')\n        weight = limit_weight * (1 - learn_rate) + start_weight * learn_rate\n        weights.append(weight)\n\n    return tuple(weights)\n</code></pre>"},{"location":"api/core/loss/#abfml.core.loss.losser.register_loss_meters","title":"<code>register_loss_meters(loss_set=None, base_meters=True)</code>","text":"<p>General-purpose meter registration function.</p> <p>Parameters:</p> Name Type Description Default <code>loss_set</code> <code>object</code> <p>A config object that contains start and limit weights for each loss term.</p> <code>None</code> <code>base_meters</code> <code>bool</code> <p>Whether to include basic meters like 'loss' and 'time'.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>OrderedDict</code> <p>A dictionary of AverageMeter instances keyed by loss type.</p> Source code in <code>abfml\\core\\loss\\losser.py</code> <pre><code>def register_loss_meters(loss_set: LossSet = None, base_meters=True):\n    \"\"\"\n       General-purpose meter registration function.\n\n       Parameters:\n           loss_set (object): A config object that contains start and limit weights for each loss term.\n           base_meters (bool): Whether to include basic meters like 'loss' and 'time'.\n\n       Returns:\n           OrderedDict: A dictionary of AverageMeter instances keyed by loss type.\n       \"\"\"\n    meters = OrderedDict()\n\n    if base_meters:\n        meters[\"loss\"] = AverageMeter(\"Loss\", fmt=\".2e\", summary_type=\"AVERAGE\")\n        meters[\"time\"] = AverageMeter(\"Time\", fmt=\"2.3f\", summary_type=\"SUM\")\n\n    if loss_set is not None:\n        # Register optional loss terms based on non-zero weights\n        optional_losses = {\n            \"energy\": (\"E_tot\", loss_set.start_energy_weight, loss_set.limit_energy_weight),\n            \"atomic_energies\": (\"Ei\", loss_set.start_ei_weight, loss_set.limit_ei_weight),\n            \"forces\": (\"Force\", loss_set.start_force_weight, loss_set.limit_force_weight),\n            \"virials\": (\"Virial\", loss_set.start_virial_weight, loss_set.limit_virial_weight)\n        }\n\n        for key, (display_name, start, limit) in optional_losses.items():\n            if start != 0.0 or limit != 0.0:\n                meters[key] = AverageMeter(display_name, fmt=\".2e\", summary_type=\"AVERAGE\")\n\n    else:\n        display_names = {\n            \"energy\": \"E_tot\",\n            \"atomic_energies\": \"Ei\",\n            \"forces\": \"Force\",\n            \"virials\": \"Virial\"\n        }\n        for key in display_names:\n            meters[key] = AverageMeter(display_names[key], fmt=\".2e\", summary_type=\"AVERAGE\")\n    return meters\n</code></pre>"},{"location":"api/core/optimizer/","title":"abfml.core.optimizer","text":""},{"location":"api/core/optimizer/#abfml.core.optimizer.optim_init.TrainSet","title":"<code>TrainSet = create_dataclass(train_set_schema)</code>  <code>module-attribute</code>","text":""},{"location":"api/core/optimizer/#abfml.core.optimizer.optim_init.optim_init","title":"<code>optim_init(model, param)</code>","text":"<p>Initialize optimizer based on param.optimizer and param.lr. Optionally uses param.weight_decay and param.momentum if provided.</p> Source code in <code>abfml\\core\\optimizer\\optim_init.py</code> <pre><code>def optim_init(model, param: TrainSet):\n    \"\"\"\n    Initialize optimizer based on param.optimizer and param.lr.\n    Optionally uses param.weight_decay and param.momentum if provided.\n    \"\"\"\n    optim_name = param.optimizer\n    lr = 0.001\n    weight_decay = getattr(param, \"weight_decay\", 0.0)\n    momentum = getattr(param, \"momentum\", 0.9)\n\n    if optim_name == \"Adam\":\n        optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay, eps=1e-5)\n    elif optim_name == \"Adamw\":\n        optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n    elif optim_name == \"sgd\":\n        optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n    elif optim_name == \"rmsprop\":\n        optimizer = torch.optim.RMSprop(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n    elif optim_name == \"adagrad\":\n        optimizer = torch.optim.Adagrad(model.parameters(), lr=lr, weight_decay=weight_decay)\n    elif optim_name == \"lbfgs\":\n        optimizer = torch.optim.LBFGS(model.parameters(), lr=lr, max_iter=20)\n    elif optim_name == \"user_defined\":\n        optimizer = None\n    else:\n        raise ValueError(f\"Unsupported optimizer type: {param.optimizer}\")\n\n    return optimizer\n</code></pre>"},{"location":"api/core/optimizer/#abfml.core.optimizer.learn_rate.LrSet","title":"<code>LrSet = create_dataclass(learning_rate_schema)</code>  <code>module-attribute</code>","text":""},{"location":"api/core/optimizer/#abfml.core.optimizer.learn_rate.adjust_lr","title":"<code>adjust_lr(lr_param, iters_step)</code>","text":"<p>Calculate the current learning rate based on schedule type.</p> Source code in <code>abfml\\core\\optimizer\\learn_rate.py</code> <pre><code>def adjust_lr(lr_param: LrSet, iters_step: int) -&gt; float:\n    \"\"\"Calculate the current learning rate based on schedule type.\"\"\"\n    if lr_param.type_name == 'exp':\n        lr = lr_param.start_lr * lr_param.decay_rate ** (iters_step / lr_param.decay_step)\n    elif lr_param.type_name == 'exp_decay':\n        lr = lr_param.start_lr * lr_param.decay_rate ** (iters_step // lr_param.decay_step)\n    else:\n        raise ValueError(f\"Unsupported learning rate type: {lr_param.type_name}\")\n\n    return max(lr, lr_param.limit_lr)\n</code></pre>"},{"location":"api/core/optimizer/#abfml.core.optimizer.learn_rate.calc_lr_param","title":"<code>calc_lr_param(lr_param, epochs, train_iters, logger_name=None)</code>","text":"<p>Calculate the decay_rate if not set. Otherwise estimate when limit_lr will be reached.</p> Source code in <code>abfml\\core\\optimizer\\learn_rate.py</code> <pre><code>def calc_lr_param(lr_param: LrSet, epochs: int, train_iters: int, logger_name: str = None):\n    \"\"\"\n    Calculate the decay_rate if not set. Otherwise estimate when limit_lr will be reached.\n    \"\"\"\n    total_iters = epochs * train_iters\n    logger = Logger(logger_name).logger if logger_name else None\n\n    if lr_param.decay_rate is None:\n        # Compute decay_rate to reach limit_lr in total_iters\n        if total_iters &gt; 0:\n            lr_param.decay_rate = (lr_param.limit_lr / lr_param.start_lr) ** (lr_param.decay_step / total_iters)\n            if logger:\n                logger.info(f\"[Auto LR] start_lr: {lr_param.start_lr:.2e}, limit_lr: {lr_param.limit_lr:.2e}, \"\n                            f\"decay_step: {lr_param.decay_step}, computed decay_rate: {lr_param.decay_rate:.4f}\")\n        else:\n            raise ValueError(\"Total training iterations (epochs * train_iters) must be greater than zero.\")\n    else:\n        # Estimate how many iterations are needed to reach limit_lr\n        try:\n            needed_iters = math.log(lr_param.limit_lr / lr_param.start_lr) / math.log(lr_param.decay_rate)\n            needed_iters *= lr_param.decay_step\n            estimated_epochs = math.ceil(needed_iters / train_iters) if train_iters &gt; 0 else float('inf')\n\n            if logger:\n                if needed_iters &lt; total_iters:\n                    logger.info(f\"[LR Info] limit_lr will be reached in ~{estimated_epochs} epochs.\")\n                else:\n                    logger.warning(f\"[LR Warning] Need ~{estimated_epochs} epochs to reach limit_lr, \"\n                                   f\"but training is set to {epochs} epochs.\")\n        except (ValueError, ZeroDivisionError) as e:\n            if logger:\n                logger.error(f\"[LR Error] Failed to compute decay trajectory: {e}\")\n</code></pre>"},{"location":"api/core/model/method/","title":"abfml.core.model","text":""},{"location":"api/core/model/method/#abfml.core.model.method.FieldModel","title":"<code>FieldModel</code>","text":"<p>               Bases: <code>Module</code>, <code>ABC</code></p> Source code in <code>abfml\\core\\model\\method.py</code> <pre><code>class FieldModel(nn.Module, ABC):\n    def __init__(self,\n                 type_map: List[int],\n                 cutoff: float,\n                 neighbor: Union[Dict[int, int],int]):\n        \"\"\"\n        Base class for field-based machine learning potential models.\n\n        Parameters\n        ----------\n        type_map : List[int]\n            A list mapping atomic species to integer indices used in the model.\n            For example, [0, 1, 2] might represent three different element types.\n\n        cutoff : float\n            Cutoff radius for neighbor searching.\n            Only atoms within this distance will be considered as neighbors.\n\n        neighbor : Union[Dict[int, int], int]\n            Maximum number of neighbors for each atom type (or a global value).\n            Used to define tensor sizes or guide model input structure.\n\n        Examples\n        --------\n        ```python\n        # Create a model with two element types, cutoff radius 6.0,\n        # and maximum 100 neighbors for element type 1.\n        model = FieldModel(type_map=[1], cutoff=6.0, neighbor=100)\n        \"\"\"\n        super(FieldModel, self).__init__()\n        self.type_map = type_map\n        self.neighbor = neighbor\n        self.cutoff = cutoff\n\n    def forward(self,\n                element_map: torch.Tensor,\n                central_atoms: torch.Tensor,\n                neighbor_indices: torch.Tensor,\n                neighbor_types: torch.Tensor,\n                neighbor_vectors: torch.Tensor,\n                n_ghost: int) -&gt; Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Main forward pass computing physical quantities\n        Args:\n            element_map: Atomic numbers mapped to indices [batch, n_atoms]\n            central_atoms: Central atom indices [batch, n_atoms]\n            neighbor_indices: Neighbor atom indices [batch, n_atoms, max_nbr]\n            neighbor_types: Neighbor atom types [batch, n_atoms, max_nbr]\n            neighbor_vectors: Relative position vectors [batch, n_atoms, max_nbr, 4]\n                              (distance, dx, dy, dz)\n            n_ghost: Number of ghost atoms in the system\n\n        Returns:\n            Tuple containing:\n            - Etot [batch, 1]\n            - Ei [batch, n_atoms, 1]\n            - force [batch, n_atoms, 3]\n            - virial tensor [batch, 9]\n            - atomic_virial [batch, n_atoms, 9]\n        \"\"\"\n        # Extract dimensions and tensor properties\n        batch, n_atoms, max_neighbor, _ = neighbor_vectors.shape\n        device, dtype = neighbor_vectors.device, neighbor_vectors.dtype\n\n        # Extract displacement vectors (dx, dy, dz) and enable autograd\n        relative_positions = neighbor_vectors[..., 1:]\n        relative_positions.requires_grad_(True)\n\n        # Recalculate distance norm for generalization and recombine into input vector\n        distance = torch.norm(relative_positions, dim=-1, keepdim=True)  # Shape: [batch, n_atoms, max_nbr, 1]\n        neighbor_vector = torch.cat([distance, relative_positions], dim=-1)  # Shape: [batch, n_atoms, max_nbr, 4]\n\n        # Call the user-defined field function to compute energy and optionally other properties\n        physics_info = self.field(\n            element_map=element_map,\n            central_atoms=central_atoms,\n            neighbor_indices=neighbor_indices,\n            neighbor_types=neighbor_types,\n            neighbor_vectors=neighbor_vector,\n            n_ghost=n_ghost\n        )\n\n        # Required outputs\n        Etot = physics_info['Etot']  # Total energy per configuration, shape: [batch, 1]\n        Ei = physics_info['Ei']  # Per-atom energy, shape: [batch, n_atoms, 1]\n\n        # Validate required outputs\n        if Etot is None or not isinstance(Etot, torch.Tensor) or Etot.shape != (batch, 1):\n            raise ValueError(\"physics_info['Etot'] must be a torch.Tensor of shape (batch, 1)\")\n        if Ei is None or not isinstance(Ei, torch.Tensor) or Ei.shape != (batch, n_atoms, 1):\n            raise ValueError(\"physics_info['Ei'] must be a torch.Tensor of shape (batch, n_atoms, 1)\")\n\n        # If force is not provided, compute it via autograd\n        if physics_info.get('Force') is None:\n            # Create a gradient mask for backpropagation\n            energy_mask: List[Optional[torch.Tensor]] = [torch.ones_like(Ei)]\n\n            # Compute gradient of Ei with respect to atomic positions\n            grad_Ei = torch.autograd.grad(\n                outputs=[Ei],\n                inputs=[relative_positions],\n                grad_outputs=energy_mask,\n                retain_graph=True,\n                create_graph=True\n            )[0]\n            assert grad_Ei is not None\n\n            # Compute force, virial tensor, and atomic-level virial from gradients\n            force, virial, atomic_virial = derive_mechanics(\n                grad_Ei=grad_Ei,\n                neighbor_vectors=neighbor_vector,\n                neighbor_indices=neighbor_indices,\n                n_ghost=n_ghost\n            )\n        else:\n            # If force is provided by field, validate its shape\n            force = physics_info['Force']\n            if not isinstance(force, torch.Tensor) or force.shape != (batch, n_atoms, 3):\n                raise ValueError(\"physics_info['Force'] must be a torch.Tensor of shape (batch, n_atoms, 3)\")\n\n            # Optional: check for virial tensor\n            virial = physics_info.get('virial')\n            if virial is not None:\n                if not isinstance(virial, torch.Tensor) or virial.shape != (batch, 3, 3):\n                    raise ValueError(\"physics_info['virial'] must be a torch.Tensor of shape (batch, 3, 3)\")\n            else:\n                virial = torch.zeros(batch, 3, 3, dtype=dtype, device=device)\n\n            # Optional: check for atomic-level virial\n            atomic_virial = physics_info.get('atomic_virial')\n            if atomic_virial is not None:\n                if not isinstance(atomic_virial, torch.Tensor) or atomic_virial.shape != (batch, n_atoms, 9):\n                    raise ValueError(\n                        \"physics_info['atomic_virial'] must be a torch.Tensor of shape (batch, n_atoms, 9)\")\n            else:\n                atomic_virial = torch.zeros(batch, n_atoms, 9, dtype=dtype, device=device)\n\n        # Return all computed quantities\n        return Etot, Ei, force, virial, atomic_virial\n\n    def field(self,\n              element_map: torch.Tensor,\n              central_atoms: torch.Tensor,\n              neighbor_indices: torch.Tensor,\n              neighbor_types: torch.Tensor,\n              neighbor_vectors: torch.Tensor,\n              n_ghost: int) -&gt; Dict[str, torch.Tensor]:\n        \"\"\"\n        Abstract method to be implemented by subclasses. This method defines the core\n        field calculation of the model, which computes atom-wise energies and optionally\n        forces and virial tensors based on atomic environments.\n\n        Parameters\n        ----------\n        element_map : torch.Tensor\n            Tensor of shape [batch, n_atoms], indicating the atomic species (element index)\n            for each atom in the batch.\n\n        central_atoms : torch.Tensor\n            Tensor of shape [batch, n_atoms], representing the type of central atoms.\n\n        neighbor_indices : torch.Tensor\n            Tensor of shape [batch, n_atoms, max_nbr], providing the indices of neighboring\n            atoms for each central atom.\n\n        neighbor_types : torch.Tensor\n            Tensor of shape [batch, n_atoms, max_nbr], indicating the atomic species\n            of neighboring atoms.\n\n        neighbor_vectors : torch.Tensor\n            Tensor of shape [batch, n_atoms, max_nbr, 4], where the last dimension represents\n            (distance, dx, dy, dz), i.e., the norm and vector components of the relative positions.\n\n        n_ghost : int\n            Number of ghost atoms (used to pad or augment neighbor lists, typically for parallelism).\n\n        Returns\n        -------\n        Dict[str, torch.Tensor]\n            A dictionary containing the computed physical quantities:\n            - 'Etot': Total energy, shape [batch, 1].\n            - 'Ei': Atomic energy per atom, shape [batch, n_atoms, 1].\n            - Optional: 'Force': Force on each atom, shape [batch, n_atoms, 3].\n            - Optional: 'virial': Virial tensor, shape [batch, 9].\n            - Optional: 'atomic_virial': virial Force on each atom, shape [batch, n_atoms, 9].\n\n        Note\n        ----\n        This method must be implemented in subclasses of `FieldModel`.\n        \"\"\"\n        return {'Etot': torch.tensor([0.0])}\n</code></pre>"},{"location":"api/core/model/method/#abfml.core.model.method.FieldModel.__init__","title":"<code>__init__(type_map, cutoff, neighbor)</code>","text":"<p>Base class for field-based machine learning potential models.</p>"},{"location":"api/core/model/method/#abfml.core.model.method.FieldModel.__init__--parameters","title":"Parameters","text":"<p>type_map : List[int]     A list mapping atomic species to integer indices used in the model.     For example, [0, 1, 2] might represent three different element types.</p> float <p>Cutoff radius for neighbor searching. Only atoms within this distance will be considered as neighbors.</p> Union[Dict[int, int], int] <p>Maximum number of neighbors for each atom type (or a global value). Used to define tensor sizes or guide model input structure.</p>"},{"location":"api/core/model/method/#abfml.core.model.method.FieldModel.__init__--examples","title":"Examples","text":"<p>```python</p>"},{"location":"api/core/model/method/#abfml.core.model.method.FieldModel.__init__--create-a-model-with-two-element-types-cutoff-radius-60","title":"Create a model with two element types, cutoff radius 6.0,","text":""},{"location":"api/core/model/method/#abfml.core.model.method.FieldModel.__init__--and-maximum-100-neighbors-for-element-type-1","title":"and maximum 100 neighbors for element type 1.","text":"<p>model = FieldModel(type_map=[1], cutoff=6.0, neighbor=100)</p> Source code in <code>abfml\\core\\model\\method.py</code> <pre><code>def __init__(self,\n             type_map: List[int],\n             cutoff: float,\n             neighbor: Union[Dict[int, int],int]):\n    \"\"\"\n    Base class for field-based machine learning potential models.\n\n    Parameters\n    ----------\n    type_map : List[int]\n        A list mapping atomic species to integer indices used in the model.\n        For example, [0, 1, 2] might represent three different element types.\n\n    cutoff : float\n        Cutoff radius for neighbor searching.\n        Only atoms within this distance will be considered as neighbors.\n\n    neighbor : Union[Dict[int, int], int]\n        Maximum number of neighbors for each atom type (or a global value).\n        Used to define tensor sizes or guide model input structure.\n\n    Examples\n    --------\n    ```python\n    # Create a model with two element types, cutoff radius 6.0,\n    # and maximum 100 neighbors for element type 1.\n    model = FieldModel(type_map=[1], cutoff=6.0, neighbor=100)\n    \"\"\"\n    super(FieldModel, self).__init__()\n    self.type_map = type_map\n    self.neighbor = neighbor\n    self.cutoff = cutoff\n</code></pre>"},{"location":"api/core/model/method/#abfml.core.model.method.FieldModel.field","title":"<code>field(element_map, central_atoms, neighbor_indices, neighbor_types, neighbor_vectors, n_ghost)</code>","text":"<p>Abstract method to be implemented by subclasses. This method defines the core field calculation of the model, which computes atom-wise energies and optionally forces and virial tensors based on atomic environments.</p>"},{"location":"api/core/model/method/#abfml.core.model.method.FieldModel.field--parameters","title":"Parameters","text":"<p>element_map : torch.Tensor     Tensor of shape [batch, n_atoms], indicating the atomic species (element index)     for each atom in the batch.</p> torch.Tensor <p>Tensor of shape [batch, n_atoms], representing the type of central atoms.</p> torch.Tensor <p>Tensor of shape [batch, n_atoms, max_nbr], providing the indices of neighboring atoms for each central atom.</p> torch.Tensor <p>Tensor of shape [batch, n_atoms, max_nbr], indicating the atomic species of neighboring atoms.</p> torch.Tensor <p>Tensor of shape [batch, n_atoms, max_nbr, 4], where the last dimension represents (distance, dx, dy, dz), i.e., the norm and vector components of the relative positions.</p> int <p>Number of ghost atoms (used to pad or augment neighbor lists, typically for parallelism).</p>"},{"location":"api/core/model/method/#abfml.core.model.method.FieldModel.field--returns","title":"Returns","text":"<p>Dict[str, torch.Tensor]     A dictionary containing the computed physical quantities:     - 'Etot': Total energy, shape [batch, 1].     - 'Ei': Atomic energy per atom, shape [batch, n_atoms, 1].     - Optional: 'Force': Force on each atom, shape [batch, n_atoms, 3].     - Optional: 'virial': Virial tensor, shape [batch, 9].     - Optional: 'atomic_virial': virial Force on each atom, shape [batch, n_atoms, 9].</p>"},{"location":"api/core/model/method/#abfml.core.model.method.FieldModel.field--note","title":"Note","text":"<p>This method must be implemented in subclasses of <code>FieldModel</code>.</p> Source code in <code>abfml\\core\\model\\method.py</code> <pre><code>def field(self,\n          element_map: torch.Tensor,\n          central_atoms: torch.Tensor,\n          neighbor_indices: torch.Tensor,\n          neighbor_types: torch.Tensor,\n          neighbor_vectors: torch.Tensor,\n          n_ghost: int) -&gt; Dict[str, torch.Tensor]:\n    \"\"\"\n    Abstract method to be implemented by subclasses. This method defines the core\n    field calculation of the model, which computes atom-wise energies and optionally\n    forces and virial tensors based on atomic environments.\n\n    Parameters\n    ----------\n    element_map : torch.Tensor\n        Tensor of shape [batch, n_atoms], indicating the atomic species (element index)\n        for each atom in the batch.\n\n    central_atoms : torch.Tensor\n        Tensor of shape [batch, n_atoms], representing the type of central atoms.\n\n    neighbor_indices : torch.Tensor\n        Tensor of shape [batch, n_atoms, max_nbr], providing the indices of neighboring\n        atoms for each central atom.\n\n    neighbor_types : torch.Tensor\n        Tensor of shape [batch, n_atoms, max_nbr], indicating the atomic species\n        of neighboring atoms.\n\n    neighbor_vectors : torch.Tensor\n        Tensor of shape [batch, n_atoms, max_nbr, 4], where the last dimension represents\n        (distance, dx, dy, dz), i.e., the norm and vector components of the relative positions.\n\n    n_ghost : int\n        Number of ghost atoms (used to pad or augment neighbor lists, typically for parallelism).\n\n    Returns\n    -------\n    Dict[str, torch.Tensor]\n        A dictionary containing the computed physical quantities:\n        - 'Etot': Total energy, shape [batch, 1].\n        - 'Ei': Atomic energy per atom, shape [batch, n_atoms, 1].\n        - Optional: 'Force': Force on each atom, shape [batch, n_atoms, 3].\n        - Optional: 'virial': Virial tensor, shape [batch, 9].\n        - Optional: 'atomic_virial': virial Force on each atom, shape [batch, n_atoms, 9].\n\n    Note\n    ----\n    This method must be implemented in subclasses of `FieldModel`.\n    \"\"\"\n    return {'Etot': torch.tensor([0.0])}\n</code></pre>"},{"location":"api/core/model/method/#abfml.core.model.method.FieldModel.forward","title":"<code>forward(element_map, central_atoms, neighbor_indices, neighbor_types, neighbor_vectors, n_ghost)</code>","text":"<p>Main forward pass computing physical quantities Args:     element_map: Atomic numbers mapped to indices [batch, n_atoms]     central_atoms: Central atom indices [batch, n_atoms]     neighbor_indices: Neighbor atom indices [batch, n_atoms, max_nbr]     neighbor_types: Neighbor atom types [batch, n_atoms, max_nbr]     neighbor_vectors: Relative position vectors [batch, n_atoms, max_nbr, 4]                       (distance, dx, dy, dz)     n_ghost: Number of ghost atoms in the system</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>Tuple containing:</p> <code>Tensor</code> <ul> <li>Etot [batch, 1]</li> </ul> <code>Tensor</code> <ul> <li>Ei [batch, n_atoms, 1]</li> </ul> <code>Tensor</code> <ul> <li>force [batch, n_atoms, 3]</li> </ul> <code>Tensor</code> <ul> <li>virial tensor [batch, 9]</li> </ul> <code>Tuple[Tensor, Tensor, Tensor, Tensor, Tensor]</code> <ul> <li>atomic_virial [batch, n_atoms, 9]</li> </ul> Source code in <code>abfml\\core\\model\\method.py</code> <pre><code>def forward(self,\n            element_map: torch.Tensor,\n            central_atoms: torch.Tensor,\n            neighbor_indices: torch.Tensor,\n            neighbor_types: torch.Tensor,\n            neighbor_vectors: torch.Tensor,\n            n_ghost: int) -&gt; Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Main forward pass computing physical quantities\n    Args:\n        element_map: Atomic numbers mapped to indices [batch, n_atoms]\n        central_atoms: Central atom indices [batch, n_atoms]\n        neighbor_indices: Neighbor atom indices [batch, n_atoms, max_nbr]\n        neighbor_types: Neighbor atom types [batch, n_atoms, max_nbr]\n        neighbor_vectors: Relative position vectors [batch, n_atoms, max_nbr, 4]\n                          (distance, dx, dy, dz)\n        n_ghost: Number of ghost atoms in the system\n\n    Returns:\n        Tuple containing:\n        - Etot [batch, 1]\n        - Ei [batch, n_atoms, 1]\n        - force [batch, n_atoms, 3]\n        - virial tensor [batch, 9]\n        - atomic_virial [batch, n_atoms, 9]\n    \"\"\"\n    # Extract dimensions and tensor properties\n    batch, n_atoms, max_neighbor, _ = neighbor_vectors.shape\n    device, dtype = neighbor_vectors.device, neighbor_vectors.dtype\n\n    # Extract displacement vectors (dx, dy, dz) and enable autograd\n    relative_positions = neighbor_vectors[..., 1:]\n    relative_positions.requires_grad_(True)\n\n    # Recalculate distance norm for generalization and recombine into input vector\n    distance = torch.norm(relative_positions, dim=-1, keepdim=True)  # Shape: [batch, n_atoms, max_nbr, 1]\n    neighbor_vector = torch.cat([distance, relative_positions], dim=-1)  # Shape: [batch, n_atoms, max_nbr, 4]\n\n    # Call the user-defined field function to compute energy and optionally other properties\n    physics_info = self.field(\n        element_map=element_map,\n        central_atoms=central_atoms,\n        neighbor_indices=neighbor_indices,\n        neighbor_types=neighbor_types,\n        neighbor_vectors=neighbor_vector,\n        n_ghost=n_ghost\n    )\n\n    # Required outputs\n    Etot = physics_info['Etot']  # Total energy per configuration, shape: [batch, 1]\n    Ei = physics_info['Ei']  # Per-atom energy, shape: [batch, n_atoms, 1]\n\n    # Validate required outputs\n    if Etot is None or not isinstance(Etot, torch.Tensor) or Etot.shape != (batch, 1):\n        raise ValueError(\"physics_info['Etot'] must be a torch.Tensor of shape (batch, 1)\")\n    if Ei is None or not isinstance(Ei, torch.Tensor) or Ei.shape != (batch, n_atoms, 1):\n        raise ValueError(\"physics_info['Ei'] must be a torch.Tensor of shape (batch, n_atoms, 1)\")\n\n    # If force is not provided, compute it via autograd\n    if physics_info.get('Force') is None:\n        # Create a gradient mask for backpropagation\n        energy_mask: List[Optional[torch.Tensor]] = [torch.ones_like(Ei)]\n\n        # Compute gradient of Ei with respect to atomic positions\n        grad_Ei = torch.autograd.grad(\n            outputs=[Ei],\n            inputs=[relative_positions],\n            grad_outputs=energy_mask,\n            retain_graph=True,\n            create_graph=True\n        )[0]\n        assert grad_Ei is not None\n\n        # Compute force, virial tensor, and atomic-level virial from gradients\n        force, virial, atomic_virial = derive_mechanics(\n            grad_Ei=grad_Ei,\n            neighbor_vectors=neighbor_vector,\n            neighbor_indices=neighbor_indices,\n            n_ghost=n_ghost\n        )\n    else:\n        # If force is provided by field, validate its shape\n        force = physics_info['Force']\n        if not isinstance(force, torch.Tensor) or force.shape != (batch, n_atoms, 3):\n            raise ValueError(\"physics_info['Force'] must be a torch.Tensor of shape (batch, n_atoms, 3)\")\n\n        # Optional: check for virial tensor\n        virial = physics_info.get('virial')\n        if virial is not None:\n            if not isinstance(virial, torch.Tensor) or virial.shape != (batch, 3, 3):\n                raise ValueError(\"physics_info['virial'] must be a torch.Tensor of shape (batch, 3, 3)\")\n        else:\n            virial = torch.zeros(batch, 3, 3, dtype=dtype, device=device)\n\n        # Optional: check for atomic-level virial\n        atomic_virial = physics_info.get('atomic_virial')\n        if atomic_virial is not None:\n            if not isinstance(atomic_virial, torch.Tensor) or atomic_virial.shape != (batch, n_atoms, 9):\n                raise ValueError(\n                    \"physics_info['atomic_virial'] must be a torch.Tensor of shape (batch, n_atoms, 9)\")\n        else:\n            atomic_virial = torch.zeros(batch, n_atoms, 9, dtype=dtype, device=device)\n\n    # Return all computed quantities\n    return Etot, Ei, force, virial, atomic_virial\n</code></pre>"},{"location":"api/core/model/method/#abfml.core.model.method.NormalModel","title":"<code>NormalModel</code>","text":"Source code in <code>abfml\\core\\model\\method.py</code> <pre><code>class NormalModel:\n    def __init__(self,\n                 normal_data,\n                 param_class: Param,\n                 normal_rate: Union[float, str] = 'auto',\n                 is_get_energy_shift: bool = False):\n        self.param_class = param_class\n        self.normal_loader = NormalModel.normal_data_loader(need_data=normal_data, normal_rate=normal_rate)\n        self.is_get_energy_shift = is_get_energy_shift\n        self.normal_data = tuple([])\n\n    def initialize(self):\n        normal_data = self.normal(normal_loader=self.normal_loader, param_class=self.param_class)\n\n        if isinstance(normal_data, tuple):\n            self.normal_data = normal_data\n        else:\n            self.normal_data = tuple([normal_data])\n\n        if self.is_get_energy_shift:\n            energy_shift = NormalModel.get_energy_shift(need_data=self.normal_loader, type_map=self.param_class.GlobalSet.type_map)\n            self.normal_data = tuple([energy_shift]) + self.normal_data\n\n    @staticmethod\n    def normal_data_loader(need_data, normal_rate: Union[float, str]) -&gt; DataLoader:\n        total_image_num = len(need_data)\n        total_indices = np.arange(total_image_num)\n        if isinstance(normal_rate, float):\n            if normal_rate &lt;= 1.0:\n                num = int(total_image_num * normal_rate + 1)\n            else:\n                raise Exception(\"rate\")\n        elif normal_rate == \"auto\":\n            if total_image_num * 0.1 &lt; 100:\n                num = total_image_num\n            else:\n                num = int(total_image_num * 0.1 + 1)\n        else:\n            raise Exception(\"rate\")\n        np.random.shuffle(total_indices)\n        normal_indices = total_indices[:num]\n        normal_data = Subset(need_data, normal_indices)\n        num_threads = torch.get_num_threads()\n        num_worker = int(num_threads / 2)\n        normal_data_loader = DataLoader(normal_data, batch_size=1, shuffle=True, num_workers=num_worker)\n\n        return normal_data_loader\n\n    @staticmethod\n    def get_energy_shift(need_data, type_map: List[int]) -&gt; List[float]:\n        ntype = len(type_map)\n        type_num = torch.zeros(ntype)\n        energy_shift = [0.0] * ntype\n        for i, image_batch in enumerate(need_data):\n            central_atoms = image_batch[\"central_atoms\"]\n            element_types = image_batch[\"element_types\"][0].to(torch.int64).tolist()\n            for i_type, element in enumerate(element_types):\n                mask = (central_atoms == element)\n                indices = type_map.index(element)\n                type_num[indices] += 1\n                try:\n                    energy = torch.mean(image_batch[\"energy\"] / image_batch[\"n_atoms\"]).item()\n                    energy_shift[indices] = energy_shift[indices] + energy\n                except KeyError:\n                    try:\n                        Ei = torch.mean(image_batch[\"atomic_energy\"][mask]).item()\n                        energy_shift[indices] = energy_shift[indices] + Ei\n                    except KeyError:\n                        energy_shift[indices] = energy_shift[indices] + np.random.uniform(-10.0, 0.0)\n\n        type_num[type_num == 0] = 1\n        for i, i_energy in enumerate(energy_shift):\n            energy_shift[i] = (i_energy / type_num[i]).item()\n\n        return energy_shift\n\n    def normal(self, normal_loader, param_class):\n        return None\n</code></pre>"},{"location":"api/core/model/method/#abfml.core.model.method.Param","title":"<code>Param</code>","text":"Source code in <code>abfml\\param\\param.py</code> <pre><code>class Param:\n    def __init__(self, input_dict: dict):\n        self.input_dict_new = {}\n        global_dict = input_dict['global_setting']\n        validator = DefaultValidatingDraft7Validator(global_set_schema)\n        validator.validate(global_dict)\n        self.GlobalSet = GlobalSet(**global_dict)\n        if isinstance(self.GlobalSet.neighbor, list):\n            self.GlobalSet.neighbor = {t: n for t, n in zip(self.GlobalSet.type_map, self.GlobalSet.neighbor)}\n\n        data_dict = input_dict['data_setting']\n        validator = DefaultValidatingDraft7Validator(data_set_schema)\n        validator.validate(data_dict)\n        self.DataSet = DataSet(**data_dict)\n\n        train_dict = input_dict['train_setting']\n        validator = DefaultValidatingDraft7Validator(train_set_schema)\n        validator.validate(train_dict)\n        self.TrainSet = TrainSet(**train_dict)\n\n        loss_dict = input_dict['loss_setting']\n        validator = DefaultValidatingDraft7Validator(loss_set_schema)\n        validator.validate(loss_dict)\n        self.LossSet = LossSet(**loss_dict)\n\n        learning_rate_dict = input_dict['learning_rate']\n        validator = DefaultValidatingDraft7Validator(learning_rate_schema)\n        validator.validate(learning_rate_dict)\n        self.LrSet = LrSet(**learning_rate_dict)\n\n        self.model_name = input_dict['model_setting']['name']\n        if input_dict['model_setting']['name'] in ['dp_se_e2_a', 'dp_se_e2_r', 'dp_se_e3']:\n            fitting_config = input_dict['model_setting']['fitting_net']\n            embedding_config = input_dict['model_setting']['descriptor']\n            self.DeepSe = DeepSe(fitting_config=fitting_config, embedding_config=embedding_config)\n        elif input_dict['model_setting']['name'] == 'BPMlp':\n            fitting_config = input_dict['model_setting']['fitting_net']\n            feature_config = input_dict['model_setting']['descriptor']\n            self.BPDescriptor = BPDescriptor(fitting_config=fitting_config, feature_config=feature_config)\n        elif input_dict['model_setting']['name'] == \"NEP\":\n            fitting_config = input_dict['model_setting']['fitting_net']\n            feature_config = input_dict['model_setting']['descriptor']\n            self.NEPParam = NEPParam(fitting_config=fitting_config, feature_config=feature_config)\n        elif input_dict['model_setting']['name'] == \"test\":\n            self.test = self.Test(input_dict[\"model_setting\"])\n        elif input_dict['model_setting']['name'] == \"user_defined\":\n            self.user_defined = self.UserDefined(input_dict[\"model_setting\"])\n\n        else:\n            raise Exception(f\"Undefined keyword: , Please read the manual!\")\n\n    class Test:\n        def __init__(self, input_dict):\n            self.input_dict = input_dict\n\n    class UserDefined:\n        config: dict = {}\n        model_path: str = \"\"\n        field_name: str = \"\"\n        normal_name: str = None\n\n        def __init__(self, config: dict):\n            if 'descriptor' in config.keys():\n                self.config = config[\"descriptor\"]\n            else:\n                raise Exception(\"Please write the custom parameters in the descriptor\")\n\n            if \"model_path\" in config.keys():\n                self.model_path = config[\"model_path\"]\n            else:\n                raise Exception(\"Undefined model path:{model_path} ?\")\n\n            self.field_name = config[\"field_name\"]\n\n            if \"normal_name\" in config.keys():\n                self.normal_name = config[\"normal_name\"]\n            else:\n                self.normal_name = None\n</code></pre>"},{"location":"api/core/model/method/#abfml.core.model.method.derive_mechanics","title":"<code>derive_mechanics(grad_Ei, neighbor_vectors, neighbor_indices, n_ghost)</code>","text":"Source code in <code>abfml\\core\\model\\math_fun.py</code> <pre><code>def derive_mechanics(\n    grad_Ei: torch.Tensor,\n    neighbor_vectors: torch.Tensor,\n    neighbor_indices: torch.Tensor,\n    n_ghost: int\n) -&gt; Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    batch, n_atoms, max_neighbors = neighbor_indices.shape\n    dtype, device = neighbor_vectors.dtype, neighbor_vectors.device\n\n    # Initialize force and per-atom virial tensors\n    force = torch.zeros(batch, n_atoms + n_ghost, 3, dtype=dtype, device=device)\n    force[:, :n_atoms, :] = grad_Ei.sum(dim=-2)  # Direct contribution from central atoms\n\n    atomic_virial = torch.zeros(batch, n_atoms + n_ghost, 9, dtype=dtype, device=device)\n\n    # Extract relative position vectors (exclude norm)\n    rel_pos = neighbor_vectors[..., 1:]  # Shape: [batch, n_atoms, max_neighbors, 3]\n\n    # Compute local virial contributions via outer product of dr and dE\n    local_virials = torch.matmul(rel_pos.unsqueeze(-1), -1 * grad_Ei.unsqueeze(-2)).reshape(batch, n_atoms, max_neighbors, 9)\n\n    # Replace invalid neighbor indices (-1) with 0\n    neighbor_indices[neighbor_indices == -1] = 0\n\n    for b in range(batch):\n        # Flatten indices and contribution values\n        neighbor_idx_b = neighbor_indices[b].view(-1).to(torch.int64)\n\n        # Force scatter-add\n        force_contrib_b = -1 * grad_Ei[b].view(-1, 3)\n        index_f = neighbor_idx_b.unsqueeze(-1).expand(-1, 3)\n        force[b] = force[b].scatter_add(0, index_f, force_contrib_b)\n\n        # Virial scatter-add\n        virial_contrib_b = local_virials[b].view(-1, 9)\n        index_v = neighbor_idx_b.unsqueeze(-1).expand(-1, 9)\n        atomic_virial[b] = atomic_virial[b].scatter_add(0, index_v, virial_contrib_b)\n\n    # Compute total virial by summing over all atoms\n    virial = atomic_virial.sum(dim=1)\n\n    return force, virial, atomic_virial\n</code></pre>"},{"location":"api/entrypoints/check/","title":"abfml.entrypoints","text":""},{"location":"api/entrypoints/check/#abfml.entrypoints.check.ABFML","title":"<code>ABFML</code>","text":"<p>               Bases: <code>Calculator</code></p> Source code in <code>abfml\\calculate.py</code> <pre><code>class ABFML(Calculator):\n    implemented_properties = ['energy', 'energies', 'forces', 'stress', 'stresses']\n\n    def __init__(self, model: str = None, dtype: str = 'float64', model_class=None, **kwargs):\n        super().__init__(**kwargs)\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.dtype = torch.float32 if dtype == \"float32\" else torch.float64\n\n        self.model = self._load_model(model, model_class)\n        self.model.to(self.dtype).to(self.device).eval()\n\n    def _load_model(self, model, model_class):\n        if model:\n            try:\n                return torch.jit.load(model, map_location=self.device)\n            except Exception:\n                try:\n                    return torch.load(model, map_location=self.device)\n                except Exception as e:\n                    raise RuntimeError(f\"Failed to load model '{model}' with jit and torch.load: {e}\")\n        elif model_class:\n            return model_class\n        else:\n            raise RuntimeError(\"No model or model_class provided.\")\n\n    def calculate(self, atoms: ase.Atoms = None, properties: List[str] = None, system_changes=all_changes):\n        super().calculate(atoms, properties, system_changes)\n\n        props = properties or self.implemented_properties\n        model = self.model\n        type_map = model.type_map\n        cutoff = model.cutoff\n        neighbor = model.neighbor\n\n        # Get neighbor info\n        element_type, central_atoms, neighbor_indices, neighbor_types, neighbor_vectors = \\\n            ReadData.calculate_neighbor(atoms=atoms, cutoff=cutoff, neighbor=neighbor)\n\n        # Prepare inputs\n        input_tensors = [\n            torch.tensor(element_type),\n            torch.tensor(central_atoms, device=self.device),\n            torch.tensor(neighbor_indices, device=self.device),\n            torch.tensor(neighbor_types, device=self.device),\n            torch.tensor(neighbor_vectors, dtype=self.dtype, device=self.device),\n            0  # In ASE, it is zero. Because neighbor of structure has not ghost atoms.\n        ]\n\n        # Model inference\n        energy, energies, forces, stress, stresses = model(*input_tensors)\n        self.results[\"energy\"] = energy.cpu().detach().numpy().item()\n        self.results[\"energies\"] = energies.cpu().detach().numpy().reshape(-1)\n        self.results[\"forces\"] = forces.cpu().detach().numpy().reshape(-1, 3)\n        if \"stress\" in properties:\n            stress = stress.cpu().detach().numpy().reshape(3, 3)\n            if any(atoms.get_pbc()):\n                stress = -0.5 * (stress * stress.T) / atoms.get_volume()\n                self.results[\"stress\"] = stress\n\n        if \"stresses\" in properties:\n            stresses = stresses.cpu().detach().numpy().reshape(-1, 3, 3)\n            if any(atoms.get_pbc()):\n                self.results[\"stresses\"] = -1.0 * stresses / atoms.get_volume()\n            else:\n                raise PropertyNotImplementedError\n</code></pre>"},{"location":"api/entrypoints/check/#abfml.entrypoints.check.check_mlff","title":"<code>check_mlff(*, model_name=None, input=None, dtype='float32', **kwargs)</code>","text":"Source code in <code>abfml\\entrypoints\\check.py</code> <pre><code>def check_mlff(\n        *,\n        model_name: Optional[str] = None,\n        input: Optional[str] = None,\n        dtype: str = \"float32\",\n        **kwargs):\n    logger = Logger(\"check.log\").logger\n    log_logo(logger=logger)\n\n    if input is not None:\n        with open(input, 'r') as file:\n            input_json = json.load(file)\n        param_class = Param(input_dict=input_json)\n        field_model_name = param_class.user_defined.field_name\n        model_path = param_class.user_defined.model_path\n        spec = importlib.util.spec_from_file_location(field_model_name, model_path)\n        module = importlib.util.module_from_spec(spec)\n        sys.modules[field_model_name] = module\n        spec.loader.exec_module(module)\n\n        field_model = getattr(module, field_model_name)\n        if param_class.user_defined.normal_name is not None:\n            logger.info(f\"| Information: Since you defined Normal, we'll read the train_file as the dataset for Normal\")\n            train_dataclass = ReadData(filename=param_class.DataSet.train_file,\n                                       cutoff=param_class.GlobalSet.cutoff,\n                                       neighbor=param_class.GlobalSet.neighbor,\n                                       file_format=param_class.DataSet.file_format)\n            train_data = ConcatDataset(train_dataclass.create_dataset())\n            normal_class = getattr(module, param_class.user_defined.normal_name)\n            normal_model = normal_class(normal_data=train_data, param_class=param_class, normal_rate=\"auto\")\n            normal_data = normal_model.normal_data\n        else:\n            normal_data = None\n        model = field_model(type_map=param_class.GlobalSet.type_map,\n                            cutoff=param_class.GlobalSet.cutoff,\n                            neighbor=param_class.GlobalSet.neighbor,\n                            config=param_class.user_defined.config,\n                            normal=normal_data)\n        calculator = ABFML(model_class=model,dtype=dtype)\n\n    elif model_name is not None:\n        calculator = ABFML(model=model_name,dtype=dtype)\n\n    else:\n        raise RuntimeError(\"model_name and input are None?\")\n\n    atomic_number = calculator.model.type_map[0]\n    element_symbol = chemical_symbols[atomic_number]\n\n    logger.info(f\"+----------------------------------- rotate symmetry check ------------------------------------+\")\n    position = np.array([[15.0, 15.0, 15.0],[13.0, 15.0, 15.0],[15.0, 15.0, 13.0]])\n    cluster = Atoms(element_symbol+'3', position)\n    cluster.calc = calculator\n    energy = [cluster.get_potential_energy()]\n    force = [cluster.get_forces()]\n    cluster.rotate(45, 'z', center='COP')\n    energy.append(cluster.get_potential_energy())\n    force.append(cluster.get_forces())\n    dF = (np.linalg.norm(force[1], axis=1) - np.linalg.norm(force[0], axis=1)).sum().item()\n    logger.info(f\"       cluster-&gt; energy: {energy[0]:&gt;.6f}, force-sum: {force[0].sum().item():&gt;.6f}\")\n    logger.info(f\"rotate cluster-&gt; energy: {energy[1]:&gt;.6f}, force-sum: {force[1].sum().item():&gt;.6f}\")\n    logger.info(f\"    difference-&gt; energy: {energy[1]-energy[0]:&gt;.6f}, force: {dF:&gt;.6f}\")\n    logger.info(f\"Information: The magnitude of the energy/and the combined force remains the same after the\")\n    logger.info(f\"             structure is rotated. Isolated system combined force should be 0.\")\n    logger.warning(f\"Part \u201cdifference\u201d should be 0. But if it is less than 1e-5, \")\n    logger.warning(f\"it may be a precision issue, otherwise check the model rotation conservation\")\n    logger.info(f\"+----------------------------------------------------------------------------------------------+\")\n\n    logger.info(f\"+-------------------------------------- periodicity check -------------------------------------+\")\n    cluster = bulk('Cu', 'fcc', a=3.62, cubic=True)\n    cluster.calc = calculator\n    energy = [cluster.get_potential_energy()]\n    force = [cluster.get_forces()]\n    cluster = cluster * (3, 3, 3)\n    cluster.calc = calculator\n    energy.append(cluster.get_potential_energy())\n    force.append(cluster.get_forces())\n    dE = energy[1] - energy[0] * 27\n    dF = np.abs(force[1][:4] - force[0]).sum().item()\n    logger.info(f\"       bulk-&gt; energy: {energy[0]:&gt;.6f}, force: {force[0].sum().item():&gt;.6f}\")\n    logger.info(f\" 3*3*3 bulk-&gt; energy: {energy[1]:&gt;.6f}, force: {force[1].sum().item():&gt;.6f}\")\n    logger.info(f\" difference-&gt; E[1]-E[0]*27: {dE:&gt;.6f},  dF: {dF:&gt;.6f}\")\n    logger.info(f\"Information: The energy of the expanded cell of a periodic structure is proportional to the number\")\n    logger.info(f\"             of times it is expanded and the force on the corresponding atoms is unchanged.\")\n    logger.warning(f\"If the difference is less than 1e-5 it may be an accuracy problem,\")\n    logger.warning(f\"otherwise the energy and force relationship should be checked.\")\n    logger.info(f\"+----------------------------------------------------------------------------------------------+\")\n\n    logger.info(f\"+------------------------------------- energy-force check -------------------------------------+\")\n    cluster = Atoms(element_symbol + '3', position)\n    cluster.calc = calculator\n    energy = [cluster.get_potential_energy()]\n    force = [cluster.get_forces()[0]]\n    cluster.positions[0] = [15.0, 15.0, 15.002]\n    energy.append(cluster.get_potential_energy())\n    force.append(cluster.get_forces()[0])\n    Fdx = -1 * (force[0][2] + force[1][2]) / 2 * 0.002\n    dE = energy[1] - energy[0]\n    logger.info(f\"       cluster-&gt; energy: {energy[0]:&gt;.6f}, force: {force[0][2].item():&gt;.6f}\")\n    logger.info(f\"  perturbation-&gt; energy: {energy[1]:&gt;.6f}, force: {force[1][2].item():&gt;.6f}\")\n    logger.info(f\"              -&gt; dE: {dE:&gt;.6f}, F * dx: {Fdx:&gt;.6f}, difference: {dE - Fdx:&gt;.6f}\")\n    logger.info(f\"Information: Force and energy should satisfy F=-dE/dx, dE=F*dx for perturbation.\")\n    logger.warning(f\"if the difference is less than 1e-5 it may be an accuracy problem,\")\n    logger.warning(f\"otherwise the energy and force relationship should be checked.\")\n    logger.info(f\"+----------------------------------------------------------------------------------------------+\")\n</code></pre>"},{"location":"api/entrypoints/main_parser/","title":"abfml.entrypoints","text":""},{"location":"api/entrypoints/main_parser/#abfml.entrypoints.main_parser.RawTextArgumentDefaultsHelpFormatter","title":"<code>RawTextArgumentDefaultsHelpFormatter</code>","text":"<p>               Bases: <code>RawTextHelpFormatter</code>, <code>ArgumentDefaultsHelpFormatter</code></p> <p>This formatter is used to print multile-line help message with default value.</p> Source code in <code>abfml\\entrypoints\\main_parser.py</code> <pre><code>class RawTextArgumentDefaultsHelpFormatter(\n    argparse.RawTextHelpFormatter, argparse.ArgumentDefaultsHelpFormatter\n):\n    \"\"\"This formatter is used to print multile-line help message with default value.\"\"\"\n</code></pre>"},{"location":"api/entrypoints/main_parser/#abfml.entrypoints.main_parser.main_parse_args","title":"<code>main_parse_args(args=None)</code>","text":"<p>Parse arguments and convert argument strings to objects.</p>"},{"location":"api/entrypoints/main_parser/#abfml.entrypoints.main_parser.main_parse_args--parameters","title":"Parameters","text":"<p>args : List[str]     list of command line arguments, main purpose is testing default option None     takes arguments from sys.argv</p>"},{"location":"api/entrypoints/main_parser/#abfml.entrypoints.main_parser.main_parse_args--returns","title":"Returns","text":"<p>argparse.Namespace     the populated namespace</p> Source code in <code>abfml\\entrypoints\\main_parser.py</code> <pre><code>def main_parse_args(args: Optional[List[str]] = None) -&gt; argparse.Namespace:\n    \"\"\"Parse arguments and convert argument strings to objects.\n\n    Parameters\n    ----------\n    args : List[str]\n        list of command line arguments, main purpose is testing default option None\n        takes arguments from sys.argv\n\n    Returns\n    -------\n    argparse.Namespace\n        the populated namespace\n    \"\"\"\n    parser = main_parser()\n    parsed_args = parser.parse_args(args=args)\n    if parsed_args.command is None:\n        parser.print_help()\n    return parsed_args\n</code></pre>"},{"location":"api/entrypoints/main_parser/#abfml.entrypoints.main_parser.main_parser","title":"<code>main_parser()</code>","text":"Source code in <code>abfml\\entrypoints\\main_parser.py</code> <pre><code>def main_parser() -&gt; argparse.ArgumentParser:\n    parser = argparse.ArgumentParser(\n        description=\"ABFML: A package for rapid building, fitting, and application of machine learning force fields\",\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n    )\n    subparsers = parser.add_subparsers(title=\"ABFML-kit subcommands\", dest=\"command\")\n\n    # **************************************   train parser start **************************************\n    parser_train = subparsers.add_parser(\n        name=\"train\",\n        parents=[],\n        help=\"train a model\",\n        formatter_class=RawTextArgumentDefaultsHelpFormatter,\n        epilog=textwrap.dedent(\n            \"\"\"\\\n        examples:\n            abfml train input.json\n            abfml train input.json --restart model.ckpt\n            abfml train input.json --init-model model.ckpt\n        \"\"\"\n        ),\n    )\n    parser_train.add_argument(\n        \"INPUT\", help=\"the input parameter file in json format.\"\n    )\n    parser_train_subgroup = parser_train.add_mutually_exclusive_group()\n    parser_train_subgroup.add_argument(\n        \"-i\",\n        \"--init_model\",\n        type=str,\n        default=None,\n        help=\"Initialize the model by the provided checkpoint.\",\n    )\n    parser_train_subgroup.add_argument(\n        \"-r\",\n        \"--restart\",\n        type=str,\n        default=None,\n        help=\"Initialize the training from the frozen model.\",\n    )\n    # **************************************   train parser end   **************************************\n\n    # **************************************   valid parser start **************************************\n    parser_valid = subparsers.add_parser(\n        name=\"valid\",\n        parents=[],\n        help=\"valid the model\",\n        formatter_class=RawTextArgumentDefaultsHelpFormatter,\n        epilog=textwrap.dedent(\n            \"\"\"\\\n        examples:\n            abfml valid -m model.pt -f /path/to/system -n 30\n        \"\"\"\n        ),\n    )\n    parser_valid.add_argument(\n        \"-m\",\n        \"--model\",\n        default=\"model.pt\",\n        type=str,\n        help=\"Valid model file to import\",\n    )\n    parser_valid.add_argument(\n        \"-s\",\n        \"--shuffle\",\n        default=False,\n        action=\"store_true\",\n        help=\"Shuffle data and randomised\",\n    )\n    parser_valid.add_argument(\n        \"-f\",\n        \"--datafile\",\n        default=None,\n        nargs=\"+\",\n        type=str,\n        help=\"The path to file of test list.\",\n    )\n    parser_valid.add_argument(\n        \"-p\",\n        \"--plot\",\n        action=\"store_true\",\n        help=\"information\",\n    )\n    parser_valid.add_argument(\n        \"-n\", \"--numb_test\",\n        default=100,\n        type=int,\n        help=\"The number of data for test\"\n    )\n\n    # **************************************   valid parser end   **************************************\n\n    # **************************************   check parser start **************************************\n    parser_check = subparsers.add_parser(\n        \"check\",\n        parents=[],\n        help=\"check the model\",\n        formatter_class=RawTextArgumentDefaultsHelpFormatter,\n        epilog=textwrap.dedent(\n            \"\"\"\\\n        examples:\n            abfml check -m model.pt \n            abfml check -i input.json -d float32\n        \"\"\"\n        ),\n    )\n\n    parser_check_subgroup = parser_check.add_mutually_exclusive_group()\n    parser_check_subgroup.add_argument(\n        \"-i\",\n        \"--input\",\n        default=None,\n        type=str,\n        help=\"the input parameter file in json format.\",\n    )\n    parser_check_subgroup.add_argument(\n        \"-m\",\n        \"--model_name\",\n        default=None,\n        type=str,\n        help=\"check model file to import\",\n    )\n    parser_check.add_argument(\n        \"-d\",\n        \"--dtype\",\n        default=\"float32\",\n        type=str,\n        help=\"Accuracy used when checking the model\",\n    )\n    # ************************************** check parser end   **************************************\n\n    return parser\n</code></pre>"},{"location":"api/entrypoints/train/","title":"abfml.entrypoints","text":""},{"location":"api/entrypoints/train/#abfml.entrypoints.train.initialize_model","title":"<code>initialize_model(param_class, init_model, logger)</code>","text":"Source code in <code>abfml\\entrypoints\\train.py</code> <pre><code>def initialize_model(param_class, init_model: Optional[str], logger):\n    # \u5c1d\u8bd5\u52a0\u8f7d\u5df2\u6709\u6a21\u578b\n    try:\n        model = torch.jit.load(init_model)\n        logger.info(f\"Successfully loaded JIT model from {init_model}\")\n    except RuntimeError:\n        try:\n            model = torch.load(init_model, map_location=\"cpu\")\n            logger.info(f\"Successfully loaded PyTorch model from {init_model}\")\n        except Exception as e:\n            raise RuntimeError(f\"Failed to load model using both torch.jit.load and torch.load: {e}\")\n\n    # \u68c0\u67e5\u6a21\u578b\u57fa\u672c\u53c2\u6570\u662f\u5426\u5339\u914d\n    if hasattr(model, 'cutoff') and model.cutoff != param_class.GlobalSet.cutoff:\n        raise ValueError(f\"Cutoff mismatch: model={model.cutoff}, config={param_class.GlobalSet.cutoff}\")\n    if hasattr(model, 'neighbor') and model.neighbor != param_class.GlobalSet.neighbor:\n        raise ValueError(f\"Neighbor mismatch: model={model.neighbor}, config={param_class.GlobalSet.neighbor}\")\n    if hasattr(model, 'type_map') and param_class.GlobalSet.type_map not in model.type_map:\n        raise ValueError(f\"type_map mismatch: config type_map {param_class.GlobalSet.type_map} not in model\")\n\n    return model\n</code></pre>"},{"location":"api/entrypoints/train/#abfml.entrypoints.train.train_loop","title":"<code>train_loop(data_load, model, optimizer, iters_step, config)</code>","text":"Source code in <code>abfml\\train\\trainer.py</code> <pre><code>def train_loop(data_load, model, optimizer, iters_step: int, config: Param):\n    logger = Logger(config.GlobalSet.logger_file).logger\n    dtype = next(model.parameters()).dtype\n    device = next(model.parameters()).device\n    batch_size = config.TrainSet.batch_size\n    criterion = torch.nn.MSELoss(reduction='sum')\n    predict_name = [\"energy\",  \"atomic_energies\", \"forces\", \"virials\"]\n    # register losses start\n    loss_dict = register_loss_meters(loss_set=config.LossSet)\n    # register losses end\n    weight_dict = {}\n    model.train()\n    for iters, image_batch in enumerate(data_load):\n        time_start = time.time()\n\n        lr_real = adjust_lr(lr_param=config.LrSet, iters_step=iters_step + iters + 1)\n        for param_group in optimizer.param_groups:\n            param_group[\"lr\"] = lr_real\n\n        n_atoms = image_batch[\"num_atoms\"][0].int().to(device)\n        neighbor_vectors = image_batch[\"neighbor_vectors\"].to(dtype=dtype, device=device)\n        element_types = image_batch[\"element_types\"][0].int().to(device)\n        central_atoms = image_batch[\"central_atoms\"].int().to(device)\n        neighbor_indices = image_batch[\"neighbor_indices\"].int().to(device)\n        neighbor_types = image_batch[\"neighbor_types\"].int().to(device)\n\n        predict_tulp = model(element_types, central_atoms, neighbor_indices, neighbor_types, neighbor_vectors, 0)\n        lr_ratio = lr_real / config.LrSet.start_lr\n        weight_tulp = calculate_weight(param=config.LossSet, learn_rate=lr_ratio)\n        weight_dict[\"energy\"] = batch_size * n_atoms ** 2\n        weight_dict[\"atomic_energies\"] = batch_size * n_atoms\n        weight_dict[\"forces\"] = batch_size * 3 * n_atoms\n        weight_dict[\"virials\"] = batch_size * 9 * n_atoms ** 2\n\n        loss = torch.tensor([0.0], dtype=dtype, device=device)\n        loss_val = torch.tensor([0.0], dtype=dtype, device=device)\n        for name in loss_dict.keys():\n            if name in predict_name:\n                try:\n                    label_val = image_batch[name].to(dtype=dtype, device=device)\n                    indices = predict_name.index(name)\n                    predict_val = predict_tulp[indices]\n                    MSE = criterion(label_val, predict_val) / weight_dict[name]\n                    loss_label_RMSE = torch.sqrt(MSE)\n                    loss_val = loss_val + loss_label_RMSE\n                    loss = loss + MSE * weight_tulp[indices]\n                    loss_dict[name].update(loss_label_RMSE.item(), batch_size)\n                except KeyError:\n                    raise Exception(f\"You are trying to train {name}, \"\n                                    f\"but the dataset doesn't include data for {name}\")\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n        time_end = time.time()\n        loss_dict[\"loss\"].update(loss_val.item(), batch_size)\n        loss_dict[\"time\"].update(time_end - time_start)\n        if iters % config.TrainSet.print_freq == 0:\n            logger.info(f\"| Train Iters:{iters + 1:&gt;6d} ,\" + ' ,'.join([str(dit) for dit in loss_dict.values()]))\n    logger.info(f\"| Train Information: \" + ' ,'.join([dit.summary() for dit in loss_dict.values()]))\n    return loss_dict\n</code></pre>"},{"location":"api/entrypoints/train/#abfml.entrypoints.train.train_mlff","title":"<code>train_mlff(*, INPUT, init_model, restart, **kwargs)</code>","text":"Source code in <code>abfml\\entrypoints\\train.py</code> <pre><code>def train_mlff(\n        *,\n        INPUT: str,\n        init_model: Optional[str],\n        restart: Optional[str],\n        **kwargs):\n    with open(INPUT, 'r') as file:\n        input_json = json.load(file)\n    param_class = Param(input_dict=input_json)\n\n    logger = Logger(param_class.GlobalSet.logger_file).logger\n    log_logo(logger=logger)\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    num_threads = torch.get_num_threads()\n    num_worker = int(num_threads / 2)\n    # torch.set_num_threads(1)\n    logger.info(f\"num_threads : {num_threads}, device : {device}\")\n    torch.set_printoptions(precision=10)\n    if param_class.DataSet.ratio is None:\n        logger.info(\"+-------------------------------------- train data file ---------------------------------------+\")\n        train_dataclass = ReadData(filename=param_class.DataSet.train_file,\n                                   cutoff=param_class.GlobalSet.cutoff,\n                                   neighbor=param_class.GlobalSet.neighbor,\n                                   file_format=param_class.DataSet.file_format)\n        log_data_info(logger, train_dataclass)\n        train_data = ConcatDataset(train_dataclass.create_dataset())\n\n        logger.info(\"+-------------------------------------- valid data file ---------------------------------------+\")\n        valid_dataclass = ReadData(filename=param_class.DataSet.valid_file,\n                                   cutoff=param_class.GlobalSet.cutoff,\n                                   neighbor=param_class.GlobalSet.neighbor,\n                                   file_format=param_class.DataSet.file_format)\n        log_data_info(logger, valid_dataclass)\n        valid_data = ConcatDataset(valid_dataclass.create_dataset())\n    else:\n        logger.info(\"+----------------------------------------- data file -------------------------------------------+\")\n        total_dataclass = ReadData(filename=param_class.DataSet.train_file,\n                                   cutoff=param_class.GlobalSet.cutoff,\n                                   neighbor=param_class.GlobalSet.neighbor,\n                                   file_format=param_class.DataSet.file_format)\n        log_data_info(logger, total_dataclass)\n        total_data = ConcatDataset(total_dataclass.create_dataset())\n        total_indices = np.arange(len(total_data))\n        if param_class.DataSet.shuffle:\n            logger.info(f\"| Because of shuffle is set to true, \")\n            logger.info(f\"| The training and validation sets will be randomly selected from all the data.\")\n            np.random.shuffle(total_indices)\n        else:\n            logger.info(f\"| Because of shuffle is set to false, preserve the order of the dataset, scaled\")\n        split_point = int(len(total_data) * param_class.DataSet.ratio)\n        train_indices, valid_indices = total_indices[:split_point], total_indices[split_point:]\n        train_data = Subset(total_data, train_indices)\n        valid_data = Subset(total_data, valid_indices)\n    train_data_load = DataLoader(train_data, batch_size=param_class.TrainSet.batch_size,\n                                 shuffle=True, num_workers=num_worker)\n    valid_data_load = DataLoader(valid_data, batch_size=param_class.TrainSet.batch_size,\n                                 shuffle=False, num_workers=num_worker)\n\n    if init_model is not None:\n        model = initialize_model(param_class, init_model, logger)\n\n    elif restart is not None:\n        check_point_class = torch.load(restart)\n        model = check_point_class[\"model\"]\n    else:\n        if param_class.model_name in [\"dp_se_e2_a\", \"dp_se_e3\", \"dp_se_e2_r\"]:\n            if param_class.model_name == \"dp_se_e2_r\":\n                from abfml.core.model.dpse import DpSe2r\n                energy_shift, std_mean = DPNormal(normal_data=train_data,\n                                                  param_class=param_class,\n                                                  normal_rate='auto',\n                                                  coordinate_matrix='r').normal_data\n                model_class = DpSe2r\n            elif param_class.model_name == \"dp_se_e2_a\":\n                from abfml.core.model.dpse import DpSe2a\n                energy_shift, std_mean = DPNormal(normal_data=train_data,\n                                                  param_class=param_class,\n                                                  normal_rate='auto',\n                                                  coordinate_matrix='a').normal_data\n                model_class = DpSe2a\n            else:\n                from abfml.core.model.dpse import DpSe3\n                energy_shift, std_mean = DPNormal(normal_data=train_data,\n                                                  param_class=param_class,\n                                                  normal_rate='auto',\n                                                  coordinate_matrix='a').normal_data\n                model_class = DpSe3\n\n            model = model_class(type_map=param_class.GlobalSet.type_map,\n                                cutoff=param_class.GlobalSet.cutoff,\n                                neighbor=param_class.GlobalSet.neighbor,\n                                fitting_config=param_class.DeepSe.fitting_config,\n                                embedding_config=param_class.DeepSe.embedding_config,\n                                energy_shift=energy_shift,\n                                std_mean=std_mean)\n\n        elif param_class.model_name == \"BPMlp\":\n            from abfml.core.model.bpmlp import BPMlp\n            energy_shift, std_mean = FeatureNormal(normal_data=train_data,\n                                                   param_class=param_class,\n                                                   normal_rate='auto',\n                                                   feature_name='BP').normal_data\n            model = BPMlp(type_map=param_class.GlobalSet.type_map,\n                          cutoff=param_class.GlobalSet.cutoff,\n                          neighbor=param_class.GlobalSet.neighbor,\n                          fit_config=param_class.BPDescriptor.fitting_config,\n                          bp_features_information=param_class.BPDescriptor.bp_features_information,\n                          bp_features_param=param_class.BPDescriptor.bp_features_param,\n                          energy_shift=energy_shift,\n                          std_mean=std_mean)\n\n        elif param_class.model_name == \"NEP\":\n            from abfml.core.model.nep import NEP\n            model = NEP(type_map=param_class.GlobalSet.type_map,\n                        cutoff=param_class.GlobalSet.cutoff,\n                        neighbor=param_class.GlobalSet.neighbor,\n                        fitting_config=param_class.NEPParam.fitting_config,\n                        feature_config=param_class.NEPParam.feature_config,\n                        energy_shift=[0,0],\n                        std_mean=[torch.tensor([0.0])])\n        elif param_class.model_name == \"user_defined\":\n            field_model_name = param_class.user_defined.field_name\n            model_path = param_class.user_defined.model_path\n            spec = importlib.util.spec_from_file_location(field_model_name, model_path)\n            module = importlib.util.module_from_spec(spec)\n            sys.modules[field_model_name] = module\n            spec.loader.exec_module(module)\n\n            field_model = getattr(module, field_model_name)\n            if param_class.user_defined.normal_name is not None:\n                normal_class = getattr(module, param_class.user_defined.normal_name)\n                normal_model = normal_class(normal_data=train_data, param_class=param_class, normal_rate=\"auto\")\n                normal_data = normal_model.normal_data\n            else:\n                normal_data = None\n            model = field_model(type_map=param_class.GlobalSet.type_map,\n                                cutoff=param_class.GlobalSet.cutoff,\n                                neighbor=param_class.GlobalSet.neighbor,\n                                config=param_class.user_defined.config,\n                                normal=normal_data)\n        else:\n            raise Exception(f\"If you define your own machine learning force field, use the name: user_defined\")\n\n    if param_class.GlobalSet.dtype == \"float32\":\n        model.to(torch.float32)\n    else:\n        model.to(torch.float64)\n    model.to(device=device)\n\n    if param_class.TrainSet.freeze_param is not None:\n        logger.info(f\"+----------------------------------- freeze or train param ------------------------------------+\")\n        namelist = [name for name, _ in model.named_parameters()]\n        if not all(name in namelist for name in param_class.TrainSet.freeze_param[1:]):\n            raise Exception(\"Error: Some parameters were not found in the model.\")\n        if param_class.TrainSet.freeze_param[0] == \"freeze\":\n            logger.info(f\"| The following parameters will be frozen.\")\n            for name, param in model.named_parameters():\n                if name in param_class.TrainSet.freeze_param[1:]:\n                    param.requires_grad = False\n        elif param_class.TrainSet.freeze_param[0] == \"train\":\n            logger.info(f\"| The following parameters will be trained.\")\n            for name, param in model.named_parameters():\n                if name not in param_class.TrainSet.freeze_param:\n                    param.requires_grad = False\n        else:\n            raise Exception(f\"Keyword {param_class.TrainSet.freeze_param[0]} error, flags only allowed freeze and train!\")\n        for name in param_class.TrainSet.freeze_param[1:]:\n            logger.info(f\"| {name:&lt;60s}\")\n        logger.info(f\"+----------------------------------------------------------------------------------------------+\")\n\n    iters_step: int = 0\n    start_epoch: int = 1\n    if restart is not None:\n        check_point_class = torch.load(restart)\n        optimizer = check_point_class[\"optimizer\"]\n        start_epoch = check_point_class[\"epoch\"]\n        iters_step = check_point_class[\"iters_step\"]\n    else:\n        optimizer = optim_init(model=model, param=param_class.TrainSet)\n        calc_lr_param(lr_param=param_class.LrSet,\n                      epochs=param_class.TrainSet.epoch,\n                      train_iters=int(len(train_data)/param_class.TrainSet.batch_size),\n                      logger_name=param_class.GlobalSet.logger_file)\n\n    logger.info(r\"########################################--Start  Train--########################################\")\n    logger.info(f\"+------------------------------------------Start Loop------------------------------------------+\")\n    time_loop_start = time.time()\n    for epoch in range(start_epoch, param_class.TrainSet.epoch + 1):\n        logger.info(f\"|-- Train Epoch:{epoch:&gt;3d}, batch size:{train_data_load.batch_size:&gt;3d}, \"\n                    f\"total iters: {len(train_data_load):&gt;6d} --------------------------------------|\")\n        train_loss_dict = train_loop(data_load=train_data_load,\n                                     model=model,\n                                     optimizer=optimizer,\n                                     iters_step=iters_step,\n                                     config=param_class)\n\n        iters_step = iters_step + len(train_data_load)\n\n        logger.info(f\"|-- Valid Epoch:{epoch:&gt;3d}, batch size:{valid_data_load.batch_size:&gt;3d}, \"\n                    f\"total iters: {len(valid_data_load):&gt;6d}                                       |\")\n        valid_loss_dict, _ = valid_loop(data_load=valid_data_load,\n                                        model=model,\n                                        logger_name=param_class.GlobalSet.logger_file,\n                                        print_freq=param_class.TrainSet.print_freq,\n                                        save_predict=False)\n\n        # ************************************** Start writing loss file ************************************** #\n        if not os.path.exists(param_class.LossSet.loss_file) or epoch == 1:\n            header = '{:&gt;8}'.format(\"Epoch\")\n            header_time = \"\"\n            for train_loss_key in train_loss_dict.keys():\n                if train_loss_key == \"time\":\n                    header_time = '{:&gt;16s}(s)'.format(\"Total_Time\")\n                else:\n                    header += '{:&gt;18s}{:&gt;18s}'.format(\"T_RMSE_\" + train_loss_dict[train_loss_key].name,\n                                                      \"V_RMSE_\" + train_loss_dict[train_loss_key].name)\n            header += header_time\n            with open(param_class.LossSet.loss_file, 'w') as fp:\n                fp.write(header)\n\n        summaries = f\"\\n{epoch:&gt;8d}\"\n        summaries_time = \"\"\n        for train_loss_key in train_loss_dict.keys():\n            if train_loss_key == \"time\":\n                total_time = '{:6.2f}'.format(train_loss_dict[\"time\"].sum + valid_loss_dict[\"time\"].sum)\n                summaries_time = '{:&gt;16s}'.format(total_time)\n            else:\n                summaries += '{:&gt;18s}'.format(format(train_loss_dict[train_loss_key].avg, '.5e'))\n                summaries += '{:&gt;18s}'.format(format(valid_loss_dict[train_loss_key].avg, '.5e'))\n        summaries += summaries_time\n        with open(param_class.LossSet.loss_file, 'a') as fp:\n            fp.write(summaries)\n        # **************************************  Stop writing loss file ************************************** #\n\n        if epoch % param_class.TrainSet.save_check == 0:\n            check_point_path = \"checkpoint_\" + str(epoch) + \".ckpt\"\n        elif epoch == param_class.TrainSet.epoch:\n            check_point_path = \"checkpoint_final.ckpt\"\n        else:\n            check_point_path = \"checkpoint_latest.ckpt\"\n\n        torch.save({\n            'epoch': epoch,\n            'iters_step': iters_step,\n            'model': model,\n            'optimizer': optimizer\n        }, check_point_path)\n    try:\n        jit_model = torch.jit.script(model)\n        jit_model.save(param_class.TrainSet.model_file)\n    except Exception as e:\n        torch.save(model, param_class.TrainSet.model_file+'.pth')\n        logger.info(f\"| Information: Failed to save the model via torchscript, will save the model by torch.save! \\n\"\n                    f\"               The model can't be loaded in lammps, but it can be used in ASE\")\n    time_loop_end = time.time()\n    logger.info(f\"+------------------------------------------ End Loop ------------------------------------------+\")\n    logger.info(f\"| Total training time:{time_loop_end - time_loop_start:8.2f}s, total training iters:{iters_step:&gt;8d}\")\n    logger.info(r\"######################################## --End  Train-- ########################################\")\n\n    return\n</code></pre>"},{"location":"api/entrypoints/valid/","title":"abfml.entrypoints","text":""},{"location":"api/entrypoints/valid/#abfml.entrypoints.valid.valid_loop","title":"<code>valid_loop(data_load, model, logger_name='valid.log', print_freq=1, save_predict=False)</code>","text":"Source code in <code>abfml\\train\\trainer.py</code> <pre><code>def valid_loop(data_load, model, logger_name: str = 'valid.log', print_freq: int = 1, save_predict: bool = False):\n    logger = Logger(logger_name).logger\n    dtype = next(model.parameters()).dtype\n    device = next(model.parameters()).device\n    batch_size = data_load.batch_size\n    criterion = torch.nn.MSELoss(reduction='sum')\n    predict_name = [\"energy\", \"atomic_energies\", \"forces\", \"virials\"]\n    predict_data = {\"energy\": [], \"atomic_energies\": [], \"forces\": [], \"virials\": []}\n    # register losses start\n    loss_dict = register_loss_meters()\n    # register losses end\n    weight_dict = {}\n    model.eval()\n    common_label = set([])\n    for iters, image_batch in enumerate(data_load):\n        time_start = time.time()\n        n_atoms = image_batch[\"num_atoms\"][0].int().to(device)\n        neighbor_vectors = image_batch[\"neighbor_vectors\"].to(dtype=dtype, device=device)\n        element_types = image_batch[\"element_types\"][0].int().to(device)\n        central_atoms = image_batch[\"central_atoms\"].int().to(device)\n        neighbor_indices = image_batch[\"neighbor_indices\"].int().to(device)\n        neighbor_types = image_batch[\"neighbor_types\"].int().to(device)\n\n        predict_tulp = model(element_types, central_atoms, neighbor_indices, neighbor_types, neighbor_vectors, 0)\n\n        weight_dict[\"energy\"] = batch_size * n_atoms ** 2\n        weight_dict[\"atomic_energies\"] = batch_size * n_atoms\n        weight_dict[\"forces\"] = batch_size * 3 * n_atoms\n        weight_dict[\"virials\"] = batch_size * 9 * n_atoms ** 2\n\n        loss_val = torch.tensor([0], dtype=dtype, device=device)\n\n        common_label = set(loss_dict.keys()).intersection(set(image_batch.keys()))\n        if len(common_label) &gt; 0:\n            for name in common_label:\n                label_val = image_batch[name].to(dtype=dtype, device=device)\n                indices = predict_name.index(name)\n                predict_val = predict_tulp[indices]\n                MSE = criterion(label_val, predict_val) / weight_dict[name]\n                loss_label_RMSE = torch.sqrt(MSE)\n                loss_val = loss_val + loss_label_RMSE\n                loss_dict[name].update(loss_label_RMSE.item(), batch_size)\n                if save_predict:\n                    predict_data[name].append(predict_val.detach())\n        else:\n            raise Exception(\"If you want to verify the accuracy of a dataset, then at least one of the labels \"\n                            \"(energy, atomic_energy, force, stress) should be present in your dataset\")\n        time_end = time.time()\n        loss_dict[\"loss\"].update(loss_val.item(), batch_size)\n        loss_dict[\"time\"].update(time_end - time_start)\n        if iters % print_freq == 0:\n            logger.info(f\"| Valid Iters:{iters + 1:&gt;6d} ,\"\n                        + ' ,'.join([str(loss_dict[label]) for label in [\"loss\", \"time\"] + list(common_label)]))\n    logger.info(f\"| Valid Information: \" +\n                ' ,'.join([loss_dict[label].summary() for label in [\"loss\", \"time\"] + list(common_label)]))\n    return loss_dict, predict_data\n</code></pre>"},{"location":"api/entrypoints/valid/#abfml.entrypoints.valid.valid_mlff","title":"<code>valid_mlff(*, model, numb_test, plot, shuffle, datafile, **kwargs)</code>","text":"Source code in <code>abfml\\entrypoints\\valid.py</code> <pre><code>def valid_mlff(\n        *,\n        model: str,\n        numb_test: int,\n        plot: bool,\n        shuffle: bool,\n        datafile: Optional[list[str]],\n        **kwargs):\n    logger = Logger(\"valid.log\").logger\n    log_logo(logger=logger)\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    num_threads = torch.get_num_threads()\n    num_worker = int(num_threads / 4)\n\n    if \".ckpt\" in model:\n        logger.info(f\"| Because file {model} has the suffix ckpt,\")\n        logger.info(f\"| we will attempt to read the model from a checkpoint-type file.\")\n        model_class = torch.load(model, weights_only=False)[\"model\"]\n    else:\n        logger.info(f\"| We will attempt to read the model from a jit_script-type file {model}.\")\n        model_class = torch.jit.load(model)\n    model_class.to(device=device)\n\n    if 'valid.input' in datafile[0] and len(datafile) == 1:\n        with open('valid.input', 'r') as file:\n            input_json = json.load(file)\n            filename_list = input_json['valid_file']\n    else:\n        filename_list = datafile\n\n    logger.info(\"+-------------------------------------- valid data file ---------------------------------------+\")\n    valid_dataclass = ReadData(filename=filename_list,\n                               cutoff=model_class.cutoff,\n                               neighbor=model_class.neighbor,\n                               file_format=None)\n    log_data_info(logger, valid_dataclass)\n    valid_data = ConcatDataset(valid_dataclass.create_dataset())\n    total_indices = np.arange(len(valid_data))\n    if shuffle:\n        logger.info(f\"| You will randomly select  {numb_test:&gt;4d} image\")\n        np.random.shuffle(total_indices)\n    else:\n        logger.info(f\"| You will use the first {numb_test:&gt;4d} image\")\n    logger.info(f\"+----------------------------------------------------------------------------------------------+\")\n    valid_indices = total_indices[:numb_test]\n    subset = Subset(valid_data, valid_indices)\n    valid_data_load = DataLoader(subset, batch_size=1, shuffle=False, num_workers=0)\n    _, predict_data = valid_loop(data_load=valid_data_load,\n                                 model=model_class,\n                                 logger_name=\"valid.log\",\n                                 print_freq=1,\n                                 save_predict=True)\n    logger.info(f\"+----------------------------------------------------------------------------------------------+\")\n    unit = {'energy': 'eV', 'forces':'eV/$\\AA$', 'virials': 'eV'}\n    # Print the Quality Of Data header\n    logger.info(f\"|{'Quality Of Data'.center(94)}|\")\n    logger.info(f\"| {'Key':&lt;16} {'DFT Mean':&gt;17} {'DFT Sigma':&gt;17} {'Predict Mean':&gt;17} {'Predict Sigma':&gt;17}     |\")\n    logger.info(f\"+----------------------------------------------------------------------------------------------+\")\n\n    # Store evaluation metrics\n    indicators = []\n\n    for predict_key in predict_data.keys():\n        if len(predict_data[predict_key]) != 0:\n            # Concatenate data\n            dft = np.concatenate([arr[predict_key].detach().numpy().flatten() for arr in valid_data_load])\n            predict = np.concatenate([arr.detach().numpy().flatten() for arr in predict_data[predict_key]])\n            difference = dft - predict\n\n            # Statistical data\n            dft_mean, dft_sigma = np.mean(dft), np.std(dft)\n            predict_mean, predict_sigma = np.mean(predict), np.std(predict)\n\n            # Calculate evaluation metrics\n            mse = np.mean(difference ** 2)\n            rmse = np.sqrt(mse)\n            mae = np.mean(np.abs(difference))\n            r2 = 1 - (np.sum(difference ** 2) / np.sum((dft - dft_mean) ** 2))\n\n            # Output Quality Of Data\n            logger.info(\n                f\"| {predict_key:&lt;16} {dft_mean:&gt;17.4e} {dft_sigma:&gt;17.4f} {predict_mean:&gt;17.4e} {predict_sigma:&gt;17.4f}     |\")\n\n            # Save metrics data for later use\n            indicators.append((predict_key, rmse, mse, mae, r2))\n\n            if plot:\n                # Plot scatter plot (DFT vs Predict)\n                from abfml.utils.chart import plot_scatter\n                plot_scatter(dft, predict, predict_key, rmse, r2, unit[predict_key])\n\n                # Plot error distribution (Difference Histogram + KDE)\n                from abfml.utils.chart import plot_error_distribution\n                plot_error_distribution(difference, predict_key, rmse, r2, unit[predict_key])\n\n                # Save data (DFT and Predict)\n                np.savetxt(f'{predict_key}.csv', np.column_stack((dft, predict)), header='DFT,Predict', delimiter=',')\n\n    logger.info(f\"+----------------------------------------------------------------------------------------------+\")\n\n    # Print the Evaluation Indicators header\n    logger.info(f\"|{'Evaluation Indicators'.center(94)}|\")\n    logger.info(f\"| {'Key':&lt;16} {'RMSE':&gt;17} {'MSE':&gt;17} {'MAE':&gt;17} {'R\u00b2':&gt;17}     |\")\n    logger.info(f\"+----------------------------------------------------------------------------------------------+\")\n\n    # Output metrics data\n    for predict_key, rmse, mse, mae, r2 in indicators:\n        logger.info(f\"| {predict_key:&lt;16} {rmse:&gt;17.4e} {mse:&gt;17.4e} {mae:&gt;17.4e} {r2:&gt;17.4f}     |\")\n\n    logger.info(f\"+----------------------------------------------------------------------------------------------+\")\n</code></pre>"},{"location":"tutorials/","title":"ABFML Tutorial","text":"<p>to be continued</p>"}]}